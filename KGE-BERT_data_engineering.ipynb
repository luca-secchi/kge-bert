{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683da5f5",
   "metadata": {},
   "source": [
    "### Autoreload Extension Activation\n",
    "\n",
    "The code above is used to activate the autoreload extension in Jupyter notebook. This extension allows for automatic reloading of modules before executing user code, thus any changes made in the modules will be reflected without the need to restart the kernel. The '2' argument allows all currently imported modules to be reloaded every time before executing the Python code typed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aabef01-87dd-4194-a288-6e01d143dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8611a54f",
   "metadata": {},
   "source": [
    "### Setting Up Modin Environment\n",
    "\n",
    "This code block is used for setting up the Modin environment. It first imports necessary libraries, determines the number of CPUs available for computation, and sets up the appropriate environment variables. Then, it configures the Modin engine to use Dask and sets the number of partitions equal to the number of CPUs. A Dask client is also created for distributed computing. Lastly, it sets the maximum number of rows to display to 30 when using Modin's pandas implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d085e1b8-c8fe-4c51-bff2-07c09d8ee415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import modin\n",
    "import modin.pandas as md\n",
    "num_cpus = multiprocessing.cpu_count() -1\n",
    "\n",
    "os.environ[\"MODIN_CPUS\"] = str(num_cpus)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "modin.config.Engine.put(\"Dask\")\n",
    "modin.config.NPartitions.put(num_cpus)\n",
    "\n",
    "from distributed import Client\n",
    "client = Client()\n",
    "print(\"Modin using %s cpus\" % modin.config.NPartitions.get())\n",
    "\n",
    "\n",
    "md.set_option('display.max_rows', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06047475",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries and Modules\n",
    "\n",
    "This section of code is responsible for importing all the necessary libraries and modules that will be used throughout the rest of the code. This includes libraries for data manipulation (like `rdfpandas`), machine learning (`xgboost`), data visualization (`matplotlib`, `seaborn`), date handling (`datetime`), and others. The `helper` module is also imported, which presumably contains custom functions to assist with data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3deac7-b01d-4902-84e9-0bbcc08ac722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.helper import *\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt   \n",
    "from xgboost import plot_importance\n",
    "import pickle \n",
    "import seaborn as sns\n",
    "from numpy import sort\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import rdfpandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87beebe-bf31-45c4-a775-f18a8337f74e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Input files \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e11597d",
   "metadata": {},
   "source": [
    "### File Paths and Constants\n",
    "\n",
    "This section of code defines the file paths for the destination data file and the mapped listings file. It also sets the date for the listings and a default random state.\n",
    "`destination_data_file`: is the filename of AirBnB tourism destination data (e.g. London), download it from https://insideairbnb.com/\n",
    "\n",
    "`listings_mapped_entities`: is the filename of the DBpedia entities extracted from each AirBnB accommodation description form file \"destination_data_file\"\n",
    "\n",
    "`dataset_name`: is the name that would be appended to the training, dev and test datasets created to train ans test KGE-BERT model\n",
    "\n",
    "`last_date`: is the date of the data dump (e.g. y=2022, m=09, d=10)\n",
    "\n",
    "`DEFAULT_RANDOM_STATE`: is the random seed used for results' reproducibility sake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8e7b76-9a70-46cd-a038-6607905b71f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_data_file = './data/london/date=20220910/listings.csv.gz'\n",
    "dataset_name = \"airbnb_london_20220910\"\n",
    "listings_mapped_entities = \"data/london/date=20220910/london_listings_mapped.csv\"\n",
    "last_date = date(2022, 9, 10)\n",
    "DEFAULT_RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b08ec1",
   "metadata": {},
   "source": [
    "### Load Destination Data\n",
    "\n",
    "The code reads a CSV file named `destination_data_file` into a pandas DataFrame called `destination_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c4618-a873-400f-a7f8-546c22dce207",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_data = pd.read_csv(destination_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d433e5",
   "metadata": {},
   "source": [
    "### Column Types Definition\n",
    "\n",
    "The code defines a dictionary `column_types` that categorizes different types of columns in a dataset into various categories such as `category_columns`, `boolean_columns`, `date_columns`, `present_absent_columns`, `to_drop_cols`, `amenities_col`, `amenities_tao_col`, `entities_col`, and `listing_type_cols`. It then creates a new category `listing_type_and_category_cols` that combines `listing_type_cols` and `category_columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64800f5-321a-423a-ab73-08b4214c6867",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = {\n",
    "    \"category_columns\" : ['neighbourhood_cleansed', 'host_response_time'],\n",
    "    \"boolean_columns\" : ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', 'has_availability', 'instant_bookable'],\n",
    "    \"date_columns\" : ['host_since', 'first_review', 'last_review'],\n",
    "    \"present_absent_columns\" : ['host_about', 'host_neighbourhood'],\n",
    "    \"to_drop_cols\" : ['neighbourhood', 'host_location', 'host_about','neighborhood_overview','host_location','bathrooms_text'],\n",
    "    \"amenities_col\" : ['amenities_v'],\n",
    "    \"amenities_tao_col\" : ['amenities_tao_v'],\n",
    "    \"entities_col\" : ['entities_v'],\n",
    "    \"listing_type_cols\" : ['property_type', 'room_type']}\n",
    "column_types[\"listing_type_and_category_cols\"] = column_types[\"listing_type_cols\"] + column_types[\"category_columns\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46845a61",
   "metadata": {},
   "source": [
    "### Loading and Mapping TAO Class Ontologies\n",
    "\n",
    "The code provides functions to load TAO (Tourist Attraction Ontology) class mappings from files and create an amenity dictionary.\n",
    "\n",
    "1. `load_tao_mappings`: This function reads mapping files for different TAO classes (location amenities, location facilities, accommodations, tourist locations) and returns dataframes for each class mapping.\n",
    "\n",
    "2. `AmenityMapper_Factory`: This function utilizes the `load_tao_mappings` function to load all TAO class mappings and creates an `AmenityMapper` for all mappings and for location amenities mappings.\n",
    "\n",
    "3. `create_amenity_dic`: This function creates a dictionary for a given list of amenities, where each amenity is linked to its corresponding class through the `AmenityMapper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40af5e11-837c-4f83-a062-37034a3bcee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tao_mappings(\n",
    "        la_mapping_file = \"class_mapping/la_ontology_mapping.csv\",\n",
    "        lf_mapping_file = \"class_mapping/lf_ontology_mapping.csv\",\n",
    "        acco_mapping_file = \"class_mapping/accommodation_ontology_mapping.csv\",\n",
    "        tl_mapping_file = \"class_mapping/tourist_location_ontology_mapping.csv\"):\n",
    "    \"\"\"Loads TAO class mappings from files.\n",
    "    Each mapping files has two columns:\n",
    "    label: contains the lower case text label associate to the TAO class\n",
    "    class: contains the camel case OWL class in the onyology \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    la_mapping_file : str\n",
    "        The mapping file complete path for location amenities' classes\n",
    "    lf_mapping_file : str\n",
    "        The mapping file complete path for location facilities' classes\n",
    "    acco_mapping_file : str\n",
    "        The mapping file complete path for accommodations' classes\n",
    "    tl_mapping_file :\n",
    "        The mapping file complete path for tourist locations' classes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of 4 pandas dataframe containing mappings for location \n",
    "    amenities, location facilities, accommodations, tourist locations\n",
    "    \"\"\"\n",
    "    \n",
    "    #### List of amenities labels vs classes extracted from ontology\n",
    "    df_ab2t_o = pd.read_csv(la_mapping_file)\n",
    "    df_ab2t_o.rename(columns={'class': 'amenity_class'}, inplace=True)\n",
    "    df_ab2t_o[\"amenity_id\"] = df_ab2t_o[\"label\"].apply(lambda e: clean_name(str(e)).lower().strip())\n",
    "    \n",
    "    #### List of l labels vs classes extracted from ontology\n",
    "    df_lf_o = pd.read_csv(lf_mapping_file)\n",
    "    df_lf_o.rename(columns={'class': 'amenity_class'}, inplace=True)\n",
    "    df_lf_o[\"amenity_id\"] = df_lf_o[\"label\"].apply(lambda e: clean_name(str(e)).lower().strip())\n",
    "    \n",
    "    #### List of l labels vs classes extracted from ontology\n",
    "    df_ac_o = pd.read_csv(acco_mapping_file)\n",
    "    df_ac_o.rename(columns={'class': 'amenity_class'}, inplace=True)\n",
    "    df_ac_o[\"amenity_id\"] = df_ac_o[\"label\"].apply(lambda e: clean_name(str(e)).lower().strip())\n",
    "    \n",
    "    #### List of l labels vs classes extracted from ontology\n",
    "    df_tl_o = pd.read_csv(tl_mapping_file)\n",
    "    df_tl_o.rename(columns={'class': 'amenity_class'}, inplace=True)\n",
    "    df_tl_o[\"amenity_id\"] = df_tl_o[\"label\"].apply(lambda e: clean_name(str(e)).lower().strip())\n",
    "    \n",
    "    return df_ab2t_o, df_lf_o, df_ac_o, df_tl_o\n",
    "\n",
    "def AmenityMapper_Factory(\n",
    "        la_mapping_file = \"class_mapping/la_ontology_mapping.csv\",\n",
    "        lf_mapping_file = \"class_mapping/lf_ontology_mapping.csv\",\n",
    "        acco_mapping_file = \"class_mapping/accommodation_ontology_mapping.csv\",\n",
    "        tl_mapping_file = \"class_mapping/tourist_location_ontology_mapping.csv\"):\n",
    "    \n",
    "    df_ab2t_o, df_lf_o, df_ac_o, df_tl_o = load_tao_mappings(la_mapping_file, lf_mapping_file, acco_mapping_file, tl_mapping_file)\n",
    "    \n",
    "    all_mappings = pd.concat([df_ab2t_o, df_lf_o, df_ac_o, df_tl_o])\n",
    "    all_matcher = AmenityMapper(all_mappings)\n",
    "    am_matcher = AmenityMapper(df_ab2t_o)\n",
    "    return all_matcher, am_matcher\n",
    "\n",
    "def create_amenity_dic(am_matcher, cleaned_amenities):\n",
    "    am_dict = {}\n",
    "    for am in cleaned_amenities:\n",
    "        am_dict[am] = am_matcher.amenity_linker(am)\n",
    "    return am_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c42712",
   "metadata": {},
   "source": [
    "### Creating Amenities Root Map\n",
    "\n",
    "This function `create_amentities_root_map` generates a mapping of amenities and their root classes. It first identifies subclasses of the root class `LocationAmenity` and then finds valid subclasses by traversing through their descendants. It then creates a generalization of classes based on their valid parents. Finally, it creates a dataframe `amentities_root_map_df` and dictionary `amentities_root_map` that represents the mapping of amenities to their root classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b400a3e-8e11-4165-a963-9cc98025ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_amentities_root_map(tao):\n",
    "    tao_classes = {}\n",
    "    root_class = tao.LocationAmenity\n",
    "    first_level_classes = set(root_class.subclasses())\n",
    "    print(\"first level classes: \", first_level_classes) \n",
    "    valid_classes = set()\n",
    "    for subc in first_level_classes:  \n",
    "        #print(\"subclasses of \", subc)\n",
    "        for c in subc.descendants():\n",
    "            valid_classes.add(c)\n",
    "            #print(\">>>  \",c)\n",
    "    #print(valid_classes)\n",
    "\n",
    "    class_generalization = {}\n",
    "    for subc in first_level_classes:  \n",
    "        #print(subc)\n",
    "        for c in subc.descendants():\n",
    "            #print(\"----\",c)\n",
    "            if c.iri.startswith(tao_base_iri):\n",
    "                parents = set(c.is_a)\n",
    "                valid_parents = parents.intersection(valid_classes) - first_level_classes\n",
    "                #print(c.name, parents, valid_parents)            \n",
    "                if len(valid_parents) == 1:\n",
    "                    class_generalization[c.name] = valid_parents.pop()\n",
    "                    #print(\"Class %s ...selected parent: %s\" % (c, class_generalization[c.name]))\n",
    "                else:\n",
    "                    class_generalization[c.name] = c\n",
    "                    #print(\"No valid parent found using \", c)\n",
    "    \n",
    "    amentities_root_map = root_sublclasses_tree(tao.LocationAmenity)\n",
    "    amentities_root_map_df = pd.DataFrame.from_dict(amentities_root_map, orient='index', columns=[\"macro_amenity\"]).reset_index(names='amenity').explode(\"macro_amenity\")\n",
    "    \n",
    "    return amentities_root_map_df, amentities_root_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068dfc26",
   "metadata": {},
   "source": [
    "### Data Preparation Function\n",
    "\n",
    "This function `prepare_data` performs a series of data cleaning and preprocessing tasks on given dataframes. It handles null values, converts data types, creates new features, and prepares the data for further analysis. The function also drops unused columns, removes rows and columns with null values, and extracts specific data from the amenities and entities columns. It then generates hot encodings for entities and places. The final output is a dictionary containing all the cleaned and processed dataframes along with some additional useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd91f0f8-51c4-4d4a-aeda-1620dc6fc5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, ct, last_date, df_ent, amentities_root_map_df, pl_min_freq = 20):\n",
    "    boolean_columns = ct[\"boolean_columns\"]\n",
    "    date_columns = ct[\"date_columns\"]\n",
    "    present_absent_columns = ct['present_absent_columns']\n",
    "    present_absent_columns_renamed = [\"pa_\"+col for col in present_absent_columns]\n",
    "    \n",
    "    # avoid side effects on dataframe\n",
    "    df = df.copy()\n",
    "    df_ent = df_ent.copy()\n",
    "    all_matcher, am_matcher = AmenityMapper_Factory()\n",
    "    \n",
    "    \n",
    "    # fill null values in columns\n",
    "    df.description = df.description.fillna(\"\").apply(lambda text: pre_process(text))\n",
    "    df.bedrooms = df.bedrooms.fillna(0)\n",
    "    df.host_response_time = df.host_response_time.fillna(\"undefined\")\n",
    "    df.host_neighbourhood = df.host_neighbourhood.fillna(\"\")\n",
    "    df.host_is_superhost = df.host_is_superhost.fillna(0)\n",
    "    df.name = df.name.fillna(\"\")\n",
    "    df.beds = df.beds.fillna(0)\n",
    "    \n",
    "    # change price into float\n",
    "    df.price = clean_price(df.price)\n",
    "    \n",
    "    # change string t/f columns into bolean\n",
    "    df[boolean_columns] = df[boolean_columns].replace(\"t\", 0).replace(\"f\", 1)\n",
    "    \n",
    "    # cast date columns to datetime and calculate days subce\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], infer_datetime_format=True, )\n",
    "    \n",
    "    df[date_columns] = df[date_columns].apply(lambda my_col: my_col.apply(lambda my_date: calc_days_since(my_date, last_date, my_col.name)))    \n",
    "    \n",
    "    # change percentages to float and fill null values with mean\n",
    "    df.host_response_rate = df.host_response_rate.str.replace(\"%\",\"\").astype(np.float64)\n",
    "    df.host_acceptance_rate = df.host_acceptance_rate.str.replace(\"%\",\"\").astype(np.float64)\n",
    "    df.host_response_rate.fillna(df.host_response_rate.mean(), inplace=True)\n",
    "    df.host_acceptance_rate.fillna(df.host_acceptance_rate.mean(), inplace=True)\n",
    "    \n",
    "    # create columns with boolean value to account for presence of other columns\n",
    "    df[present_absent_columns_renamed] = df[present_absent_columns].apply(lambda my_col: my_col.apply(lambda text: int(type(text) is str)))    \n",
    "    \n",
    "    # drop unused columns\n",
    "    df = df.drop(columns=ct[\"to_drop_cols\"])\n",
    "    \n",
    "    ## remove completely null columns\n",
    "    df = df.dropna(axis=1, how = 'all')\n",
    "    \n",
    "    ## remove rows with null values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    df['amenities_v'] = df['amenities'].apply(lambda am: json.loads(am))\n",
    "    df['amenities_all_v'] = df['amenities'].apply(lambda am: json.loads(am)).apply(lambda am_list: list(set(am_list)))\n",
    "    frequent_amenities = property_frequencies(df['amenities_all_v'])\n",
    "    df['amenities_freq_v'] = df['amenities_all_v'].apply(lambda am_list: list(set(am_list).intersection(frequent_amenities)))\n",
    "    \n",
    "    distinct_amenities = list(df['amenities_all_v'].explode().unique())\n",
    "    cleaned_amenities = set([clean_name(am) for am in distinct_amenities])\n",
    "    \n",
    "    am_dict = create_amenity_dic(am_matcher, cleaned_amenities)\n",
    "    \n",
    "    df['amenities_tao_v'] = df['amenities_all_v'].apply(lambda x: remap_amenities(x, am_dict))\n",
    "    df['amenities_tao_classes_v'] = df['amenities_all_v'].apply(lambda x: remap_amenities(x, am_dict, output='class'))\n",
    "    \n",
    "    amenity_classes_for_listings = pd.DataFrame(df[['id','amenities_tao_classes_v']].explode('amenities_tao_classes_v'))\n",
    "    macro_amenity_classes_for_listings = amenity_classes_for_listings.merge(amentities_root_map_df, left_on='amenities_tao_classes_v', right_on='amenity', how=\"left\")[[\"id\",\"macro_amenity\"]]\n",
    "    macro_amenity_classes_for_listings[\"tot\"] = 1\n",
    "    macro_amenity_features = macro_amenity_classes_for_listings.reset_index().groupby(['id', 'macro_amenity'])['tot'].aggregate('sum').unstack()\n",
    "    macro_amenity_features.fillna(0, inplace=True)\n",
    "    macro_amenity_features.reset_index()\n",
    "    \n",
    "    \n",
    "    # DBpedia entities from descriptions\n",
    "    df_ent.drop_duplicates(subset=['id'], inplace=True)\n",
    "    df_ent['entities_v'] = df_ent['entities'].apply(lambda e: converter(str(e)))\n",
    "    df_ent.id.astype(int)\n",
    "    frequent_entities = property_frequencies(df_ent['entities_v']) \n",
    "    all_entities = property_frequencies(df_ent['entities_v'], min_freq = 0) \n",
    "    df = df.merge(df_ent[['id', 'entities_v']], left_on='id', right_on='id', how='left')\n",
    "    \n",
    "    ## Note df['entities_v'] has all entities with repetitions\n",
    "    df['entities_all_v'] = df['entities_v'].apply(lambda am_list: deb(am_list))\n",
    "    df['entities_freq_v'] = df['entities_all_v'].apply(lambda am_list: list(set(am_list).intersection(frequent_entities)))\n",
    "    \n",
    "    ## DBpedia entities of type Place from descriptions\n",
    "    ent_places = df_ent['entities'].apply(lambda e: converter(str(e), filter=\"place\"))\n",
    "    frequent_places = property_frequencies(ent_places, min_freq = pl_min_freq)\n",
    "    all_places = property_frequencies(ent_places, min_freq = 0) \n",
    "    \n",
    "    df_all_places = df['places_all_v'] = df['entities_all_v'].apply(lambda am_list: list(set(am_list).intersection(all_places)))\n",
    "    df_freq_places = df['places_freq_v'] = df['entities_all_v'].apply(lambda am_list: list(set(am_list).intersection(frequent_places)))\n",
    "    \n",
    "    print(len(all_places), len(frequent_places), len(frequent_entities)) \n",
    "    \n",
    "    #Hot encodings\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df_all_entities_filtered_exploded = pd.DataFrame(mlb.fit_transform(df['entities_all_v']), columns=mlb.classes_, index=df.index)\n",
    "    \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df_freq_entities_filtered_exploded = pd.DataFrame(mlb.fit_transform(df['entities_freq_v']), columns=mlb.classes_, index=df.index)\n",
    "    \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df_all_places_exploded = pd.DataFrame(mlb.fit_transform(df_all_places), columns=mlb.classes_, index=df.index)\n",
    "    df_freq_places_exploded = pd.DataFrame(mlb.fit_transform(df_freq_places), columns=mlb.classes_, index=df.index)\n",
    "    \n",
    "    \n",
    "    #df_listing_type_exploded = pd.get_dummies(X_temp, columns=listing_type_cols)\n",
    "    \n",
    "    output = {\n",
    "        \"df_data\": df,\n",
    "        \"df_entities\": df_ent,\n",
    "        \"df_all_places\": df_all_places,\n",
    "        \"df_freq_places\": df_freq_places,\n",
    "        \"frequent_entities\": frequent_entities,\n",
    "        \"all_entities\": all_entities,\n",
    "        \"amenities_lookup\": am_dict,\n",
    "        \"frequent_places\": frequent_places,\n",
    "        \"all_places\": all_places,\n",
    "        \"df_freq_places\": df_freq_places,\n",
    "        \"df_all_entities_filtered_exploded\": df_all_entities_filtered_exploded,\n",
    "        \"df_freq_entities_filtered_exploded\": df_freq_entities_filtered_exploded,\n",
    "        \"df_all_places_exploded\": df_all_places_exploded,\n",
    "        \"df_freq_places_exploded\": df_freq_places_exploded,\n",
    "        \"df_macro_amenity_classes_for_listings\": macro_amenity_classes_for_listings\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2c6e4d",
   "metadata": {},
   "source": [
    "### Loading Ontologies\n",
    "\n",
    "This section of code is responsible for loading two different ontologies: `tao.rdf` and `skos.rdf`. It sets up the world environment, specifies the file paths and base IRIs for the ontologies, and then loads them into the world environment. The namespaces for each ontology are also defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a21d20f-e03b-4fda-a9b1-92cb8b7cb2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = World()\n",
    "tao_file = \"ontologies/tao.rdf\" \n",
    "tao_base_iri = \"http://purl.org/tao/ns#\"\n",
    "tao_ontology = world.get_ontology(tao_file).load()\n",
    "skos_file = './ontologies/skos.rdf' \n",
    "skos_ontology = world.get_ontology(skos_file).load()\n",
    "skos = skos_ontology.get_namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "tao = tao_ontology.get_namespace(tao_base_iri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92a10f",
   "metadata": {},
   "source": [
    "### Creation of Amenities Root Map\n",
    "\n",
    "The provided code block is responsible for creating an amenities root map. This map is generated by invoking the `create_amentities_root_map` function with `tao` as the input argument. The results are stored in two variables: `amentities_root_map_df` and `amentities_root_map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e34842-9124-43c5-b6ce-12c2cbe8f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "amentities_root_map_df, amentities_root_map = create_amentities_root_map(tao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df7251c",
   "metadata": {},
   "source": [
    "### Loading and Renaming DataFrame Columns\n",
    "\n",
    "This line of code is responsible for loading a CSV file into a pandas DataFrame named `df_ent`. It also renames the column originally named \"source_id\" to \"id\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8dc78-9bbc-4834-b7ac-060172b5693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ent = pd.read_csv(listings_mapped_entities, sep = \",\").rename(columns={\"source_id\": \"id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d026217",
   "metadata": {},
   "source": [
    "### Data Cleaning and Filtering\n",
    "\n",
    "The code block above is responsible for cleaning a DataFrame (`df_ent`). It first counts the total number of lines. Then it removes the lines where the `id` is either not available or is equal to 'id'. After this, it tries to convert the `id` to an integer type, and removes any rows where this conversion is not possible. Finally, it calculates and prints the number of lines that were removed during this cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe73e7c-474a-440c-af75-13f7fe186d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter wrong lines\n",
    "num_lines = df_ent.shape[0]\n",
    "df_ent = df_ent[(df_ent.id != 'id') & (df_ent.id.notna())] \n",
    "df_ent[\"id1\"] = pd.to_numeric(df_ent.id, errors='coerce', downcast=\"integer\")\n",
    "df_ent = df_ent[df_ent.id1.notna()]\n",
    "df_ent[\"id\"] = df_ent.id.astype(int)\n",
    "df_ent.drop(columns=[\"id1\"], inplace=True)\n",
    "removed_lines = num_lines - df_ent.shape[0]\n",
    "print(\"Removed corrupted lines\", removed_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724c889e",
   "metadata": {},
   "source": [
    "### Data Preparation and Storage\n",
    "\n",
    "This piece of code handles the preparation and storage of data. It provides the option to load previously computed data from a file or to prepare data from scratch. If the data is prepared from scratch, it also offers the option to save the newly prepared data to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3888ff9-6fde-4f6d-8e07-4e8860e98117",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_file = False ### Change to True to load a previous computation\n",
    "store_to_file = True\n",
    "check_pickle = True\n",
    "filename = \"checkpoints/prepared_data.pickle\"\n",
    "\n",
    "if load_from_file:\n",
    "    print(\"Loading data from file\", filename)\n",
    "    with open(filename, 'rb') as handle:\n",
    "        prepared_data = pickle.load(handle)\n",
    "else:\n",
    "    print(\"Preparing data from scratch\")\n",
    "    prepared_data = prepare_data(destination_data.copy(), column_types, last_date, df_ent, amentities_root_map_df)\n",
    "    if store_to_file:\n",
    "        print(\"Saving prepared data to file\", filename)\n",
    "        with open(filename, 'wb') as handle:\n",
    "            pickle.dump(prepared_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb4fd1a",
   "metadata": {},
   "source": [
    "### Checking Pickle File Integrity\n",
    "\n",
    "This code block checks the integrity of a Pickle file. It compares the loaded Pickle data with the original data using the DeepDiff library. If there are any differences, they are printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0fdc9f-d3b3-4c2a-8707-428a10e1dd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_pickle:\n",
    "    ## to check for pickle content with respect to the original one\n",
    "    from deepdiff import DeepDiff\n",
    "    with open(filename, 'rb') as handle:\n",
    "        loaded_data = pickle.load(handle)\n",
    "    diff = DeepDiff(prepared_data, loaded_data)\n",
    "    print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206e56b-6914-40a5-b718-ae3d71cf67cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a TAO entity matcher from descriptions\n",
    "The matcher is created using original TAO mappings and new mappings extracted from amenity metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ad002",
   "metadata": {},
   "source": [
    "### Mapping Labels to Classes\n",
    "\n",
    "This section of the code is responsible for loading class mappings from TAO (Tourism Analytics Ontology), preparing new class mappings from amenity metadata, and then combining these mappings. It also appends the base IRI (Internationalized Resource Identifier) from TAO to the 'amenity_class' column of the combined dataframe. Finally, it uses this combined dataframe to create an instance of the AmenityMapper class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd816d-0dd3-49fe-82d8-998398f43eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ab2t_o, df_lf_o, df_ac_o, df_tl_o = load_tao_mappings() ## label to class mappings from TAO \n",
    "am_dict = prepared_data[\"amenities_lookup\"] ## new label to class mappings from amenity metadata\n",
    "lookup_labels_from_data = pd.DataFrame.from_dict(am_dict, orient=\"index\", columns=[\"tao_label\", \"amenity_class\"]).reset_index().rename(columns={'index': 'label'})\n",
    "all_lookup = [df_ab2t_o, df_lf_o, df_ac_o, df_tl_o]\n",
    "all_lookup_df = pd.concat(all_lookup, axis = 0).dropna(axis=0)\n",
    "all_lookup_df[\"amenity_class\"] = tao.base_iri + all_lookup_df[\"amenity_class\"]\n",
    "print(tao.base_iri)\n",
    "am_matcher = AmenityMapper(all_lookup_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc61b0",
   "metadata": {},
   "source": [
    "### TAO Entity Extractor Testing\n",
    "\n",
    "This section of code is dedicated to testing the TAO (Text Analysis Operations) entity extractor. It does so by preprocessing a selected text from the \"description\" field of the \"destination_data\" dataset, then applying the amenity_linker_multi function to extract entities. The raw and processed text, as well as the extraction results, are then printed for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7eed1-80af-4eda-abbe-a0032e711a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### test TAO entity extractor \n",
    "i = 4\n",
    "raw_text = destination_data[\"description\"][i]\n",
    "text = pre_process(raw_text)\n",
    "res = am_matcher.amenity_linker_multi(text)\n",
    "print(raw_text)\n",
    "print(text)\n",
    "\n",
    "print(res)\n",
    "#matches = self.__matcher(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19934820-8903-494c-a836-8e2cac906319",
   "metadata": {},
   "source": [
    "## Extract all TAO entities and merge them with DBpedia entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e60d533",
   "metadata": {},
   "source": [
    "### Entity Merging Function\n",
    "\n",
    "This function, `merge_entities`, is designed to merge two lists of entities, `tao_entities` and `dbp_entities`. It first checks if each input is a list, returning the other if one is not. Then it iterates over `tao_entities`, storing the first and last word of each entity. If `dbp_entities` is a list, it does the same for each of its entities, creating sets of words for each. If there's an intersection between the two sets, the current `tao_entity` is discarded. Otherwise, it's kept. The function finally returns a list combining the kept `tao_entities` and all `dbp_entities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eb89fa-01dd-4cab-a93e-4d77bd9be8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_entities(tao_entities: list, dbp_entities:list):\n",
    "    if type(dbp_entities) is not list:\n",
    "        return tao_entities\n",
    "    if type(tao_entities) is not list:\n",
    "        return dbp_entities\n",
    "    \n",
    "    keep_tao = []\n",
    "    for _, tao_ent in enumerate(tao_entities):\n",
    "        tao_first_word = tao_ent[\"surface_word_pos\"][0]\n",
    "        tao_last_word = tao_ent[\"surface_word_pos\"][1]\n",
    "        keep = True\n",
    "        if type(dbp_entities) is list:\n",
    "            for _, dbp_ent in enumerate(dbp_entities):\n",
    "                dbp_first_word = dbp_ent[\"surface_word_pos\"][0]\n",
    "                dbp_last_word = dbp_ent[\"surface_word_pos\"][1]  ### last position is excluded\n",
    "                tao_words = set(range(tao_first_word, tao_last_word))\n",
    "                dbp_words = set(range(dbp_first_word, dbp_last_word))             \n",
    "\n",
    "                if len(tao_words.intersection(dbp_words)) > 0:\n",
    "                    keep = False\n",
    "                    break\n",
    "\n",
    "        if keep:\n",
    "            #print(\"keeping: \", tao_ent[\"surface_form\"])\n",
    "            keep_tao.append(tao_ent)\n",
    "        else:\n",
    "            #print(\"discarding:\", tao_ent,\"collision with \", dbp_ent)\n",
    "            pass\n",
    "    \n",
    "    return keep_tao + dbp_entities\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0717b0cd",
   "metadata": {},
   "source": [
    "### Extracting and Merging TAO and DBpedia Entities\n",
    "\n",
    "The function `extract_tao_entities` takes a dataframe as input, removes any null texts, and applies the TAO entity matcher to each description to extract TAO entities. \n",
    "\n",
    "The function `merge_tao_and_dbp_entities` merges the dataframe containing TAO entities with the dataframe containing DBpedia entities. It then removes rows with no DBpedia or TAO entities. Finally, it merges the DBpedia entities with the TAO entities, giving precedence to DBpedia entities if the surface forms overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3ce0b7-7fd7-4b64-8b3b-dbfdf4fbbbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tao_entities(data_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ## we must use the same text used with DBpedia spotlight in order to align surface form word positions during merge\n",
    "    df_tao_entities_from_text = data_df.dropna(axis = 0) ## remove null texts\n",
    "    #df_tao_entities_from_text = df_tao_entities_from_text[0:100] ## for testing, comment when processing all data\n",
    "    ## we use the tao entoty matcher to process each description and extract TAO entities from it\n",
    "    data_df[\"tao_entities\"] = data_df.description.apply(lambda raw_text: am_matcher.amenity_linker_multi(pre_process(raw_text)))\n",
    "    return data_df\n",
    "\n",
    "def merge_tao_and_dbp_entities(df_tao: pd.DataFrame, df_dbp: pd.DataFrame) -> pd.DataFrame:\n",
    "    ## merge the dataframe with DBpedia entities with the dataframe with TAO entities\n",
    "    df_tao_dbpedia_entities =  df_tao[[\"id\",\"tao_entities\"]].merge(df_dbp[[\"id\",\"entities_v\"]], left_on=\"id\", right_on=\"id\", how=\"outer\").rename(columns={\"entities_v\": \"dbp_entities\"})\n",
    "    ## remove rows with no DBpedia entities and no TAO endf_tao_entities_from_texttity found\n",
    "    df_tao_dbpedia_entities = df_tao_dbpedia_entities[(df_tao_dbpedia_entities.astype(str)['dbp_entities'] != '[]') & (~df_tao_dbpedia_entities.tao_entities.isna())]\n",
    "    ## merge DBpedia entities with TAO entities giving precedence to DBpedia entities if the surface forms overlap\n",
    "    df_tao_dbpedia_entities[\"merged_entities\"] = df_tao_dbpedia_entities.apply(lambda row: merge_entities(row[\"tao_entities\"], row[\"dbp_entities\"]), axis=1)\n",
    "    \n",
    "    return df_tao_dbpedia_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561da6df",
   "metadata": {},
   "source": [
    "### Entity Extraction and Storage\n",
    "\n",
    "This script handles the extraction and storage of TAO and DBpedia entities. \n",
    "\n",
    "- The first set of variables determine whether the program should load entities from a file or prepare them from scratch.\n",
    "- If `load_from_file` is `True`, the script loads TAO and DBpedia entities from a specified file. If `load_from_file_just_tao` is `True`, it also loads TAO entities extracted from text from a separate file.\n",
    "- If `load_from_file` is `False`, the script prepares TAO and DBpedia entities from scratch. It extracts TAO entities from all listing descriptions and merges them with entities extracted using DBpedia spotlight. It uses Modin dataframes to parallelize this process.\n",
    "- After preparing the entities, if `store_to_file` is `True`, the script saves the TAO and DBpedia entities to a file. If `store_to_file_just_tao` is `True`, it also saves just the TAO entities to a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74748747-58ab-4894-8f8a-e22758adb550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_from_file = False ### Change to True to load a previous computation\n",
    "load_from_file_just_tao = False\n",
    "store_to_file = True\n",
    "store_to_file_just_tao = True\n",
    "check_pickle = True\n",
    "filename = \"checkpoints/dbpedia_tao_entities_for_kbert.pickle\"\n",
    "tao_entities_parquet = \"checkpoints/tao_entities_from_text_for_bert_kg.parquet\"\n",
    "if load_from_file:\n",
    "    print(\"Loading TAO and DBpedia entities from file\", filename)\n",
    "    with open(filename, 'rb') as handle:\n",
    "        df_tao_dbpedia_entities = pickle.load(handle)\n",
    "\n",
    "    if load_from_file_just_tao:\n",
    "        print(\"Loading Just TAO entities extracted from text from file\", tao_entities_parquet)\n",
    "        df_tao_entities_from_text = pd.read_parquet(tao_entities_parquet)\n",
    "    \n",
    "else:\n",
    "    print(\"Preparing TAO and DBpedia entities from scratch\")\n",
    "    ## find TAO entities in all listings description and merge them with entites extracted using DBpedia spotlight\n",
    "    \n",
    "    ### We use Modin dataframes to parallelize\n",
    "    df_descriptions = md.DataFrame(destination_data[[\"id\", \"description\"]].copy().dropna(axis=0))\n",
    "    md_tao_entities_from_text = extract_tao_entities(df_descriptions)\n",
    "    df_tao_entities_from_text = md_tao_entities_from_text._to_pandas()\n",
    "    # Create column with vector of TAO classes found in descriptions\n",
    "    df_tao_entities_from_text.drop_duplicates(subset=['id'], inplace=True)\n",
    "    df_tao_entities_from_text['tao_entities_v'] = df_tao_entities_from_text['tao_entities'].apply(lambda e: converter(e))\n",
    "    df_tao_entities_from_text.id.astype(int)\n",
    "    df_tao_entities_from_text.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    md_dbp_ent = md.DataFrame(prepared_data[\"df_entities\"].copy()[[\"id\",\"entities\"]]) ## get dbpedia entities from prepared data\n",
    "    md_dbp_ent[\"entities_v\"] = md_dbp_ent['entities'].apply(lambda e: converter(str(e), full=True)) ## convert text to array   \n",
    "    df_dbp_ent = md_dbp_ent._to_pandas()\n",
    "    df_tao_dbpedia_entities = merge_tao_and_dbp_entities(df_tao_entities_from_text, df_dbp_ent)\n",
    "    if store_to_file:\n",
    "        print(\"Saving TAO and DBpedia entities to file\", filename)\n",
    "        with open(filename, 'wb') as handle:\n",
    "            pickle.dump(df_tao_dbpedia_entities, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    if store_to_file_just_tao:\n",
    "        print(\"Saving just TAO entities to file\", tao_entities_parquet)\n",
    "        df_tao_entities_from_text.to_parquet(tao_entities_parquet)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc119fa-316a-4bbf-9f41-5a70d4f24222",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare balanced datasets and train classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e4dba4",
   "metadata": {},
   "source": [
    "### Extracting DataFrame from Prepared Data\n",
    "\n",
    "The following line of code is used to extract a DataFrame named \"df_data\" from the prepared_data dictionary and assign it to the variable \"lc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45735777-e96f-4124-810b-a6fd2dd3c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = prepared_data[\"df_data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0d65b2",
   "metadata": {},
   "source": [
    "### Histogram Plot of Description Length\n",
    "\n",
    "The given code snippet is creating a histogram plot of the description lengths. It first pre-processes the text in the 'description' column of the 'lc' dataframe, splits it into words, counts the number of words, and then plots a histogram. The title of the histogram is 'Description length in words'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d26f7-d975-40fa-b332-1c0d93e2b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = lc.description.apply(lambda s: len(pre_process(s).split(' ')))\n",
    "distr.hist()\n",
    "plt.title('Description length in words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a033c",
   "metadata": {},
   "source": [
    "### Column Type Definitions\n",
    "\n",
    "The code above is defining different types of columns in a dataset. These include:\n",
    "- `category_columns`: These are columns with categorical data.\n",
    "- `boolean_columns`: These are columns with boolean values.\n",
    "- `date_columns`: These are columns with date values.\n",
    "- `present_absent_columns`: These columns indicate the presence or absence of a certain attribute.\n",
    "- `amenities_col`: This column contains information about the amenities of a listing.\n",
    "- `amenities_tao_col`: This column contains transformed amenities data.\n",
    "- `entities_col`: This column contains entity data.\n",
    "- `listing_type_cols`: These columns contain information about the type of the listing.\n",
    "- `listing_type_and_category_cols`: This is a combination of the `listing_type_cols` and `category_columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8137b84b-7e9a-4eae-a0ba-f5195242d37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns = column_types[\"category_columns\"] #['neighbourhood_cleansed', 'host_response_time']\n",
    "boolean_columns = column_types[\"boolean_columns\"] #['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', 'has_availability', 'instant_bookable']\n",
    "date_columns = column_types[\"date_columns\"] #['host_since', 'first_review', 'last_review']\n",
    "present_absent_columns = column_types[\"present_absent_columns\"] #['host_about', 'host_neighbourhood']\n",
    "amenities_col = column_types[\"amenities_col\"] #['amenities_v']\n",
    "amenities_tao_col = column_types[\"amenities_tao_col\"] #['amenities_tao_v']\n",
    "entities_col = column_types[\"entities_col\"] #['entities_v']\n",
    "listing_type_cols = column_types[\"listing_type_cols\"] #['property_type', 'room_type']\n",
    "listing_type_and_category_cols = listing_type_cols + category_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3646dc4f",
   "metadata": {},
   "source": [
    "### Data Preprocessing for Price Prediction\n",
    "\n",
    "This code performs data preprocessing for price prediction. It first calculates the mean and standard deviation of the prices. Then, it filters out data points that are more than two standard deviations away from the mean. Finally, it creates two lists of column names: one for numerical columns that are not related to review scores or price (the target variable), and one for the excluded columns that start with 'review_scores' or 'price'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fa5f88-dca7-4dbd-8518-718f92f11b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_mean = lc.price.mean()\n",
    "price_std = lc.price.std()\n",
    "price_prepared_data_df = lc[lc.price < price_mean + 2 * price_std]\n",
    "price_numeric_cols = [col for col in list(price_prepared_data_df.columns) if not ( col.startswith('review_scores') or col.startswith('price')) and price_prepared_data_df[col].dtype != np.dtype('O')]  ## exclude columns about reviews scores and price that we want to predict\n",
    "price_numeric_excluded_cols = [col for col in list(price_prepared_data_df.columns) if ( col.startswith('review_scores') or col.startswith('price')) and price_prepared_data_df[col].dtype != np.dtype('O')]  ## excluded columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5f616",
   "metadata": {},
   "source": [
    "### Overview of Code for Excluding Numeric Columns in Price Data\n",
    "\n",
    "The variable `price_numeric_excluded_cols` is used to store the column names of the price data that are not numeric. This is useful for data preprocessing where we need to separate numeric and non-numeric data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c7e296-8f2c-4188-8cb6-020b8878f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_numeric_excluded_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd81dda5",
   "metadata": {},
   "source": [
    "### Data Preparation for 'availability_365' Analysis\n",
    "\n",
    "This code block is used for preparing the data for analysis related to 'availability_365'. \n",
    "\n",
    "- The first line filters out null values in the 'availability_365' column of the dataset.\n",
    "- The next two lines create lists of column names. The first list includes all numeric columns that are not related to 'availability', while the second list includes all numeric columns that are related to 'availability'. These lists are created for further data processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf142725-cd38-4af4-9f88-2db85634e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "avail365_prepared_data_df = lc[lc.availability_365.notnull()]\n",
    "avail365_numeric_cols = [col for col in list(avail365_prepared_data_df.columns) if not ( col.startswith('availability')) and avail365_prepared_data_df[col].dtype != np.dtype('O')]  ## exclude columns about availability count that we want to predict\n",
    "avail365_numeric_excluded_cols = [col for col in list(avail365_prepared_data_df.columns) if ( col.startswith('availability')) and avail365_prepared_data_df[col].dtype != np.dtype('O')]  ## excluded columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b225bbf4",
   "metadata": {},
   "source": [
    "### Overview of `avail365_numeric_excluded_cols` Variable\n",
    "\n",
    "This variable `avail365_numeric_excluded_cols` is likely used to store the column names of a dataset that are excluded from computations, specifically those columns with numeric data type and associated with the availability of a resource over a 365-day period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92211db7-0a3b-4987-bd89-cd02336c72d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "avail365_numeric_excluded_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bba378",
   "metadata": {},
   "source": [
    "### Data Preparation and Column Selection\n",
    "\n",
    "The code calculates the mean and standard deviation of the number of reviews. It then prepares the data by excluding records with a number of reviews less than the mean plus twice the standard deviation. Finally, it generates two lists of column names: one excluding review-related columns with non-object data types, and the other specifically including only those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19028ffa-442e-49d3-8a0c-4de846929ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrw_mean = lc.price.mean() ### Here we have an error that has no influence in results because price mean and std are greater than number of reviews mean and std (so we are taking more data)\n",
    "nrw_std = lc.price.std()\n",
    "nrw_prepared_data_df = lc[lc.number_of_reviews_ltm < nrw_mean + 2 * nrw_std]\n",
    "num_reviews_numeric_cols = [col for col in list(nrw_prepared_data_df.columns) if not ( col.startswith('review_score') or \"review\" in col) and nrw_prepared_data_df[col].dtype != np.dtype('O')]  ## exclude columns about reviews\n",
    "num_reviews_numeric_excluded_cols = [col for col in list(nrw_prepared_data_df.columns) if ( col.startswith('review_score') or \"review\" in col) and nrw_prepared_data_df[col].dtype != np.dtype('O')]  ## exclude columns about reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c4735",
   "metadata": {},
   "source": [
    "### Data Cleaning: Exclusion of Non-Numeric Columns\n",
    "\n",
    "The variable `num_reviews_numeric_excluded_cols` likely refers to a list or array of columns in a dataset that have been excluded due to their non-numeric nature. This is typically done in data cleaning to prepare the dataset for statistical analysis or machine learning models, which often require numeric input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b37f6-f956-466f-81dd-5eb95a27121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reviews_numeric_excluded_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7896ea1",
   "metadata": {},
   "source": [
    "### Preparing Data for Review Score Prediction\n",
    "\n",
    "This section of the code is preparing the data to be used for predicting review scores. It creates two lists of column names from the DataFrame `rev_score_prepared_data_df`. The first list, `rev_score_numeric_cols`, includes all columns that do not start with 'review_scores' and are not of type object. The second list, `rev_score_numeric_excluded_cols`, includes all columns that do start with 'review_scores' and are not of type object. These two lists will be used to select the relevant features for the prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e0278-dfc4-4383-98cf-bd1bb11f206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_score_prepared_data_df = lc\n",
    "rev_score_numeric_cols = [col for col in list(rev_score_prepared_data_df.columns) if not col.startswith('review_scores') and rev_score_prepared_data_df[col].dtype != np.dtype('O')]  ## exclude columns about reviews scores that we want to predict\n",
    "rev_score_numeric_excluded_cols = [col for col in list(rev_score_prepared_data_df.columns) if col.startswith('review_scores') and rev_score_prepared_data_df[col].dtype != np.dtype('O')]  ## excluded columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc90512f",
   "metadata": {},
   "source": [
    "### Code Documentation: rev_score_numeric_excluded_cols\n",
    "\n",
    "This section of code seems to be defining a variable or function named `rev_score_numeric_excluded_cols`. However, without the actual code, it's hard to provide a meaningful comment. In general, this could be used to store or manipulate data where numeric values are excluded from a review score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff5bd56-3b48-4839-a829-01dc8aa46702",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_score_numeric_excluded_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5255c395",
   "metadata": {},
   "source": [
    "### Intersection of Numeric Columns\n",
    "\n",
    "The following code is used to find the common numeric columns in different data sets. It uses the `set.intersection()` method to identify the shared numeric columns between `price_numeric_cols`, `avail365_numeric_cols`, `num_reviews_numeric_cols`, and `rev_score_numeric_cols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c114f04-3cbb-4407-9b14-743655d9420a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "common_numeric_cols = set.intersection(set(price_numeric_cols),set(avail365_numeric_cols), set(num_reviews_numeric_cols), set(rev_score_numeric_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77027f4",
   "metadata": {},
   "source": [
    "### Set Difference Operation on Numeric Columns\n",
    "\n",
    "This line of code is performing a set difference operation between `price_numeric_cols` and `common_numeric_cols`. It returns a set that includes the numeric columns in `price_numeric_cols` that are not present in `common_numeric_cols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005410fe-c66e-4f74-abab-4319d3674f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(price_numeric_cols) - common_numeric_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d49886e",
   "metadata": {},
   "source": [
    "### Subtraction of Sets in Python\n",
    "\n",
    "The given line of code performs the operation of subtracting one set from another in Python. It subtracts the `common_numeric_cols` set from the `avail365_numeric_cols` set. The result will be a new set that contains elements present in `avail365_numeric_cols` but not in `common_numeric_cols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31829123-a10f-43d5-86a3-09604fed7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(avail365_numeric_cols) - common_numeric_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0703e86",
   "metadata": {},
   "source": [
    "### Difference Between Numeric Columns\n",
    "\n",
    "The given line of code is used to find the difference between two sets: `num_reviews_numeric_cols` and `common_numeric_cols`. It returns a set that includes items present in `num_reviews_numeric_cols` but not in `common_numeric_cols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efbd787-5e86-499e-a48a-6e5f580776b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(num_reviews_numeric_cols) - common_numeric_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83746620",
   "metadata": {},
   "source": [
    "### Set Difference Operation on Numeric Columns\n",
    "\n",
    "The given line of code performs a set difference operation between two sets: `rev_score_numeric_cols` and `common_numeric_cols`. It returns a set that contains all the elements present in `rev_score_numeric_cols` but not in `common_numeric_cols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ce17f3-81bb-48c0-81b4-3635e82ba4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(rev_score_numeric_cols) - common_numeric_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d640b106",
   "metadata": {},
   "source": [
    "### Scoring Metrics Definition\n",
    "\n",
    "This code block is responsible for defining the scoring metrics to be used for model evaluation. It includes F1 score, macro precision, and macro recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799df767-1420-49ee-8616-d3275ec4b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'f1': 'f1',\n",
    "           'prec_macro': 'precision_macro',\n",
    "           'rec_macro': 'recall_macro'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8598ce",
   "metadata": {},
   "source": [
    "### Configuration Settings\n",
    "\n",
    "The code block above specifies the configuration settings for a program. It includes flags for loading targets from a file, storing targets to a file, enabling plots, and the filename for storing the targets data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232f9e4-b8af-4497-8c97-eabf8261322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_targets_from_file = False ### Change to True to load a previous computation\n",
    "store_targets_to_file = True\n",
    "enable_plots = False\n",
    "targets_filename = \"checkpoints/targets_data.pickle\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c02bbb",
   "metadata": {},
   "source": [
    "### XGBClassifier Model Initialization\n",
    "\n",
    "The code initializes an XGBClassifier model with 'mlogloss' as the evaluation metric and importance type set to \"weight\". The XGBClassifier is a part of the XGBoost library which is used for gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783c8ac-78a1-4343-a359-8914342fda15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(eval_metric='mlogloss', importance_type=\"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b621eb1",
   "metadata": {},
   "source": [
    "### Loading Targets Data\n",
    "The code checks if `load_targets_from_file` is `True`. If so, it loads the targets data from a specified file (`targets_filename`) using the `pickle` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3877f655-3948-45d0-9600-8af513378376",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_targets_from_file:\n",
    "    print(\"Loading targets data from file\", targets_filename)\n",
    "    with open(targets_filename, 'rb') as handle:\n",
    "        targets = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f53493",
   "metadata": {},
   "source": [
    "### Feature Selection and Model Complexity Reduction\n",
    "\n",
    "The code block above is primarily concerned with feature selection and reduction of model complexity. \n",
    "\n",
    "1. It first checks whether target data needs to be computed from scratch. If so, it prepares the data for four different targets: price, availability over 365 days, number of reviews, and review score. \n",
    "\n",
    "2. It then generates a balanced dataset for each target and stores the balanced data, target vector, and excluded data in the respective target dictionary. \n",
    "\n",
    "3. It subsequently selects only the numeric columns from the balanced data and removes certain irrelevant columns. The accuracy of the model is then determined by reducing the features. \n",
    "\n",
    "4. If plots are enabled, it generates a heatmap showing the correlation between features and a plot showing the accuracy of the model against the number of features. \n",
    "\n",
    "5. It manually defines accuracy cut-offs for each target and determines the threshold for which the accuracy falls below the cut-off. \n",
    "\n",
    "6. Finally, it updates the target dictionary with the columns to keep for each target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be06e14-36d7-4946-a5f8-32050e84b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identify the most relevant features to reduce the model's complexity.\n",
    "\n",
    "if not load_targets_from_file:\n",
    "    print(\"Computing target data from scratch\")\n",
    "    targets = {\n",
    "        \"price\": {\n",
    "            \"target_col\": \"price\",\n",
    "            \"prepared_data\": price_prepared_data_df,\n",
    "            \"threshold\": price_prepared_data_df.price.median(),\n",
    "            \"safe\": price_prepared_data_df.price.median()/100*3,\n",
    "            \"numeric_cols\": price_numeric_cols,\n",
    "        },\n",
    "        \"avail365\": {\n",
    "            \"target_col\": \"availability_365\",\n",
    "            \"prepared_data\": avail365_prepared_data_df,\n",
    "            \"threshold\": 0.5,  ## zero availability vs non zero availability for 365 days\n",
    "            \"safe\": 0.1,\n",
    "            \"numeric_cols\": avail365_numeric_cols\n",
    "\n",
    "        },\n",
    "        \"num_reviews\": {\n",
    "            \"target_col\": \"number_of_reviews_ltm\",\n",
    "            \"prepared_data\": rev_score_prepared_data_df,\n",
    "            \"threshold\": 0.5,  ### we try to predict if the listing is reviewed (which means it vas visited) or not\n",
    "            \"safe\": 0.1,\n",
    "            \"numeric_cols\": num_reviews_numeric_cols\n",
    "        },\n",
    "        \"rev_score\": {\n",
    "            \"target_col\": \"review_scores_value\",\n",
    "            \"prepared_data\": nrw_prepared_data_df,\n",
    "            \"threshold\": 4.5,  ### we try to predict if the review rate (from 0 to 5) is greater of 4.5\n",
    "            \"safe\": 0.1,\n",
    "            \"numeric_cols\": rev_score_numeric_cols\n",
    "        },    \n",
    "    }\n",
    "\n",
    "    for t in targets.keys():\n",
    "        balanced_data, y, excluded_df = produce_balanced_dataset_safe(\n",
    "            targets[t][\"prepared_data\"], targets[t][\"threshold\"], targets[t][\"safe\"], col = targets[t][\"target_col\"])\n",
    "        targets[t][\"balanced_data\"] = balanced_data.dropna(axis=1, how = 'all').dropna()\n",
    "        targets[t][\"y\"] = targets[t][\"balanced_data\"][\"__y__\"].copy()\n",
    "        targets[t][\"balanced_data\"] = targets[t][\"balanced_data\"].drop(columns=[\"__y__\"])\n",
    "\n",
    "        targets[t][\"excluded_data\"] = excluded_df\n",
    "        \n",
    "    for t in targets.keys():\n",
    "    #for t in [\"rev_score\"]:\n",
    "        print(\"Target:\",t, flush=True)\n",
    "        numeric_cols = targets[t][\"numeric_cols\"]\n",
    "        data_ok = targets[t][\"balanced_data\"].copy()[numeric_cols]  ## let's take just numeric columns\n",
    "        y = targets[t][\"y\"] ## target vector\n",
    "        X = data_ok.drop(['id', 'scrape_id', 'host_id'], axis = 1)\n",
    "        print(\"X:\", X.shape, flush=True)\n",
    "        print(\"y:\", y.shape, flush=True)\n",
    "        res = accuracy_by_feature_reduction(X, y, model)\n",
    "        targets[t][\"results\"] = res\n",
    "\n",
    "        if enable_plots:\n",
    "            df_corr = X.corr()\n",
    "            df_corr_high = df_corr[df_corr>0.1]\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(data=df_corr_high, cmap=\"Oranges\", annot=False,  linewidths=0.1, linecolor=\"gray\",xticklabels=True, yticklabels=True)\n",
    "            plt.title('%s features correlation' % t)\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(res[\"partial_num_features\"], res[\"partial_accuracies\"])\n",
    "            plt.title('%s accuracy vs features' % t)\n",
    "            plt.show()      \n",
    "        \n",
    "        \n",
    "    ### Accuracy cut-offs are defined manually inspecting the accuracy by num features curves\n",
    "    targets[\"price\"][\"acc_cutoff\"] = 0.75\n",
    "    targets[\"avail365\"][\"acc_cutoff\"] = 0.8\n",
    "    targets[\"num_reviews\"][\"acc_cutoff\"] = 0.7\n",
    "    targets[\"rev_score\"][\"acc_cutoff\"] = 0.65\n",
    "\n",
    "    for t in targets.keys():\n",
    "        p_acc = pd.Series(targets[t][\"results\"][\"partial_accuracies\"])\n",
    "        p_thr = pd.Series(targets[t][\"results\"][\"thresholds\"])\n",
    "        thresh = p_thr[p_acc < targets[t][\"acc_cutoff\"]].max()\n",
    "        full_model = targets[t][\"results\"][\"full_model\"]\n",
    "        keep_cols = full_model.feature_names_in_[full_model.feature_importances_ <  thresh]\n",
    "\n",
    "        ## update target dictionary with columns to keep\n",
    "        targets[t][\"base_cols\"] = keep_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb466cff",
   "metadata": {},
   "source": [
    "### Saving Target Data to File\n",
    "\n",
    "This code block checks if the `store_targets_to_file` flag is enabled. If true, it saves the `targets` data to a file named `targets_filename` using Python's `pickle` module for serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c861d592-d453-449e-bf66-9d8144b0bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if store_targets_to_file:\n",
    "    print(\"Saving target data to file\", targets_filename)\n",
    "    with open(targets_filename, 'wb') as handle:\n",
    "        pickle.dump(targets, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed047d8-b349-410d-9892-20f15ac652dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate lookup files for vector embeddings using hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddba54b",
   "metadata": {},
   "source": [
    "### Creating Compact Lookup Function\n",
    "\n",
    "This function `create_lookup_compact` is used to create a lookup dictionary, a numpy array representation, and a final DataFrame for the given data. It performs several tasks:\n",
    "\n",
    "1. It checks if a target dataset is provided, if not, it uses the prepared data.\n",
    "2. It hot encodes the listing types in the data.\n",
    "3. It transforms the amenities into a binary form using MultiLabelBinarizer.\n",
    "4. It creates a final DataFrame by joining the original data with the hot encoded and binary transformed data.\n",
    "5. It drops the 'name' column from the final DataFrame.\n",
    "6. It converts the hot encoded columns to a numpy array.\n",
    "7. It packs the numpy array into a new numpy array with a float64 data type.\n",
    "8. It creates a lookup dictionary where each key is the 'id' from the final DataFrame and the value is the corresponding row from the packed numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdc8c2a-93d5-42fd-8a85-3d32db4e05df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_compact(X_full, prepared_data):\n",
    "    if X_full is None: ## if a we don't have target data we produce the lookup for all prepared data\n",
    "        X_full = prepared_data[\"df_data\"]\n",
    "    df_listing_type_exploded = pd.get_dummies(X_full[listing_type_cols], columns=listing_type_cols) ## sf with listing type hot encoding\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df_amenities_tao_exploded = pd.DataFrame(mlb.fit_transform(X_full['amenities_tao_v']), columns=mlb.classes_, index=X_full.index)\n",
    "    final_df = X_full[[\"id\",\"name\"]].copy()\n",
    "    final_df = final_df.\\\n",
    "        join(df_listing_type_exploded, rsuffix=\"_lt\").\\\n",
    "        join(prepared_data[\"df_freq_entities_filtered_exploded\"], rsuffix=\"_db\").\\\n",
    "        join(df_amenities_tao_exploded, rsuffix=\"_am\")\n",
    "    final_df.drop(columns={\"name\"}, inplace=True)\n",
    "\n",
    "    ## Convert hot encoded colums to numpy array  \n",
    "    l = final_df.iloc[:,1:]\n",
    "    V = l.to_numpy(dtype=bool)\n",
    "    Vpack = np.packbits(V, axis = 1).astype('float64')/255\n",
    "    lookup = {}\n",
    "    for i,id in enumerate(list(final_df[\"id\"])):\n",
    "        lookup[str(id)] = Vpack[i,:]\n",
    "    \n",
    "    return lookup, Vpack, final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89001c",
   "metadata": {},
   "source": [
    "### Creating Lookup Amenities Database\n",
    "\n",
    "This function `create_lookup_amenities_dbp__text_dbp` is used to create a lookup dictionary and a numpy array from a given prepared data. The function performs several operations:\n",
    "\n",
    "1. It extracts the data frame from the prepared data.\n",
    "2. It performs one-hot encoding on the listing type columns.\n",
    "3. It uses MultiLabelBinarizer to transform the 'amenities_dbp_classes_v' column.\n",
    "4. It creates a final dataframe by joining several dataframes based on different suffixes and drops the 'name' column.\n",
    "5. It converts the hot encoded columns to a numpy array.\n",
    "6. It creates a lookup dictionary where each key is an 'id' and its value is the corresponding row from the numpy array. \n",
    "\n",
    "The function returns the lookup dictionary, the numpy array, and the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d70e0d5-fe03-4bae-86f9-9749e95d0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_amenities_dbp__text_dbp(prepared_data):\n",
    "    X_full = prepared_data[\"df_data\"]\n",
    "    df_listing_type_exploded = pd.get_dummies(X_full[listing_type_cols], columns=listing_type_cols) ## sf with listing type hot encoding\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df_amenities_dbp_exploded = pd.DataFrame(mlb.fit_transform(X_full['amenities_dbp_classes_v']), columns=mlb.classes_, index=X_full.index)\n",
    "    print(\"Hot encoding size for amenities mapped to DBpedia:\",df_amenities_dbp_exploded.shape[1])\n",
    "    final_df = X_full[[\"id\",\"name\"]].copy()\n",
    "    final_df = final_df.\\\n",
    "        join(df_listing_type_exploded, rsuffix=\"_lt\").\\\n",
    "        join(prepared_data[\"df_freq_entities_filtered_exploded\"], rsuffix=\"_db\").\\\n",
    "        join(df_amenities_dbp_exploded, rsuffix=\"_am\")\n",
    "    final_df.drop(columns={\"name\"}, inplace=True)\n",
    "\n",
    "    ## Convert hot encoded colums to numpy array \n",
    "    l = final_df.iloc[:,1:]\n",
    "    V = l.to_numpy(dtype=bool)\n",
    "    lookup = {}\n",
    "    for i,id in enumerate(list(final_df[\"id\"])):\n",
    "        lookup[str(id)] = V[i,:]\n",
    "    \n",
    "    return lookup, V, final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef563df",
   "metadata": {},
   "source": [
    "### Creating Lookup for Amenities and Text Database\n",
    "\n",
    "This function `create_lookup_amenities_tao__text_dbp` is used to create a lookup for amenities and text data. It first checks if the target data is available, if not, it uses the prepared data. Then, it performs one-hot encoding on the listing type columns. The amenities are binarized and added to the dataframe. The final dataframe is created by joining the original dataframe with the exploded dataframes. The name column is dropped and the remaining columns are converted to a numpy array. A lookup dictionary is created where each key is an id and the value is the corresponding row from the numpy array. The function returns the lookup dictionary, the numpy array, and the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0114fe-7920-4c96-bb87-85592a1179d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_amenities_tao__text_dbp(X_full, prepared_data):\n",
    "    if X_full is None: ## if a we don't have target data we produce the lookup for all prepared data\n",
    "        X_full = prepared_data[\"df_data\"]\n",
    "    df_listing_type_exploded = pd.get_dummies(X_full[listing_type_cols], columns=listing_type_cols) ## sf with listing type hot encoding\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df_amenities_tao_exploded = pd.DataFrame(mlb.fit_transform(X_full['amenities_tao_v']), columns=mlb.classes_, index=X_full.index)\n",
    "    final_df = X_full[[\"id\",\"name\"]].copy()\n",
    "    final_df = final_df.\\\n",
    "        join(df_listing_type_exploded, rsuffix=\"_lt\").\\\n",
    "        join(prepared_data[\"df_freq_entities_filtered_exploded\"], rsuffix=\"_db\").\\\n",
    "        join(df_amenities_tao_exploded, rsuffix=\"_am\")\n",
    "    final_df.drop(columns={\"name\"}, inplace=True)\n",
    "\n",
    "    ## Convert hot encoded colums to numpy array \n",
    "    l = final_df.iloc[:,1:]\n",
    "    V = l.to_numpy(dtype=bool)\n",
    "    lookup = {}\n",
    "    for i,id in enumerate(list(final_df[\"id\"])):\n",
    "        lookup[str(id)] = V[i,:]\n",
    "    \n",
    "    return lookup, V, final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d4c600",
   "metadata": {},
   "source": [
    "### Creating Lookup Amenities and Text TAO\n",
    "\n",
    "This function `create_lookup_amenities_tao__text_tao` generates a lookup dictionary and two dataframes. It first hot encodes the listing types and amenities from the prepared data. It then creates a dataframe for TAO entities and merges it with the original dataframe. The final dataframe is then converted to a numpy array, and a lookup dictionary is created mapping each id to its corresponding numpy array. The function returns the lookup dictionary, the numpy array, and the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de984b-e451-4827-890a-fa8bee17c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_amenities_tao__text_tao(prepared_data, df_entites_tao):\n",
    "    X_full = prepared_data[\"df_data\"]\n",
    "    df_listing_type_exploded = pd.get_dummies(X_full[listing_type_cols], columns=listing_type_cols) ## sf with listing type hot encoding\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df_amenities_tao_exploded = pd.DataFrame(mlb.fit_transform(X_full['amenities_tao_v']), columns=mlb.classes_, index=X_full.index)\n",
    "    \n",
    "    df_entites_tao_exploded = pd.DataFrame(mlb.fit_transform(df_entites_tao['tao_entities_v']), columns=mlb.classes_, index=df_entites_tao.index)\n",
    "    final_df_entites_tao = df_entites_tao.join(df_entites_tao_exploded)\n",
    "    final_df_entites_tao.head()\n",
    "    final_df_entites_tao.drop(columns={\"tao_entities_v\"}, inplace=True)\n",
    "    \n",
    "    final_df = X_full[[\"id\",\"name\"]].copy()\n",
    "    final_df = final_df.\\\n",
    "        join(df_listing_type_exploded, rsuffix=\"_lt\").\\\n",
    "        join(df_amenities_tao_exploded, rsuffix=\"_am\")\n",
    "    final_df.drop(columns={\"name\"}, inplace=True)\n",
    "\n",
    "    final_df = final_df.merge(final_df_entites_tao, on = 'id', how = 'left').fillna(0)\n",
    "\n",
    "    ## Convert hot encoded colums to numpy array \n",
    "    l = final_df.iloc[:,1:]\n",
    "    V = l.to_numpy(dtype=bool)\n",
    "    lookup = {}\n",
    "    for i,id in enumerate(list(final_df[\"id\"])):\n",
    "        lookup[str(id)] = V[i,:]\n",
    "    \n",
    "    return lookup, V, final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3b2bc",
   "metadata": {},
   "source": [
    "### Creating a Lookup Text Database\n",
    "\n",
    "This function, `create_lookup_text_dbp`, is designed to create a lookup text database from prepared data. It first extracts the necessary data from the input, joins relevant data frames, and removes unnecessary columns. Then, it converts hot encoded columns to a numpy array. Finally, it uses a loop to create a lookup dictionary where each key is an ID and the value is the corresponding row from the numpy array. The function returns the lookup dictionary, the numpy array, and the final data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9013e7de-f754-492d-9b0f-a1bc7368f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_text_dbp(prepared_data):\n",
    "    X_full = prepared_data[\"df_data\"]\n",
    "    final_df = X_full[[\"id\",\"name\"]].copy()\n",
    "    final_df = final_df.join(prepared_data[\"df_freq_entities_filtered_exploded\"], rsuffix=\"_db\")\n",
    "    final_df.drop(columns={\"name\"}, inplace=True)\n",
    "\n",
    "    ## Convert hot encoded colums to numpy array \n",
    "    l = final_df.iloc[:,1:]\n",
    "    V = l.to_numpy(dtype=bool)\n",
    "    lookup = {}\n",
    "    for i,id in enumerate(list(final_df[\"id\"])):\n",
    "        lookup[str(id)] = V[i,:]\n",
    "    \n",
    "    return lookup, V, final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ffebd",
   "metadata": {},
   "source": [
    "### Creating Lookup Text for TAO Entities\n",
    "\n",
    "This function, `create_lookup_text_tao`, is used to create a lookup table for TAO (The Art of) entities. It takes as input a preprocessed dataset and a DataFrame of TAO entities. It uses the `MultiLabelBinarizer` from `sklearn` to one-hot encode the TAO entities. The function then merges this with the original dataset, and converts the one-hot encoded columns to a numpy array. The final output is a dictionary where each key-value pair represents an ID and its corresponding one-hot encoded TAO entity vector, along with the numpy array of the one-hot encoded TAO entities and the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb02463-e9c0-4f4f-b1b5-ffce319b44ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_text_tao(prepared_data, df_entites_tao):\n",
    "    X_full = prepared_data[\"df_data\"]\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df_entites_tao_exploded = pd.DataFrame(mlb.fit_transform(df_entites_tao['tao_entities_v']), columns=mlb.classes_, index=df_entites_tao.index)\n",
    "    final_df_entites_tao = df_entites_tao.join(df_entites_tao_exploded)\n",
    "    final_df_entites_tao.drop(columns={\"tao_entities_v\"}, inplace=True)\n",
    "    \n",
    "    final_df = X_full[[\"id\",\"name\"]].copy()\n",
    "    final_df = final_df.merge(final_df_entites_tao, on = 'id', how = 'left').fillna(0)\n",
    "    final_df.drop(columns={\"name\"}, inplace=True)\n",
    "\n",
    "    ## Convert hot encoded colums to numpy array \n",
    "    l = final_df.iloc[:,1:]\n",
    "    V = l.to_numpy(dtype=bool)\n",
    "    lookup = {}\n",
    "    for i,id in enumerate(list(final_df[\"id\"])):\n",
    "        lookup[str(id)] = V[i,:]\n",
    "    \n",
    "    return lookup, V, final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2de2c",
   "metadata": {},
   "source": [
    "### Amenity Lookup Table Creation\n",
    "\n",
    "This function `create_lookup_amenities_no_kg` generates a lookup table for amenities of property listings. It first identifies frequently used amenities, then creates binary vectors for each listing to indicate the presence of these amenities. The function also hot encodes the listing type and combines this information into a final dataframe. The dataframe is then converted into a numpy array and a lookup dictionary is created for each listing ID. The function returns this lookup dictionary, the numpy array, and the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039f850-57f9-4a2e-b582-cf375ef6b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_amenities_no_kg(prepared_data, min_freq = 5):\n",
    "    X_full = prepared_data[\"df_data\"].copy()\n",
    "    frequent_amenities = property_frequencies(X_full['amenities_all_v'], min_freq = min_freq) ## find the amenities used more than min_freq in all listings\n",
    "    X_full['amenities_freq_v'] = X_full['amenities_all_v'].apply(lambda am_list: list(set(am_list).intersection(frequent_amenities))) ## for each listing only store the amenities in the frequent_amenity list\n",
    "    df_listing_type_exploded = pd.get_dummies(X_full[listing_type_cols], columns=listing_type_cols) ## sf with listing type hot encoding\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df_amenities_no_kg_exploded = pd.DataFrame(mlb.fit_transform(X_full['amenities_freq_v']), columns=mlb.classes_, index=X_full.index)\n",
    "    final_df = X_full[[\"id\",\"name\"]].copy()\n",
    "    final_df = final_df.\\\n",
    "        join(df_listing_type_exploded, rsuffix=\"_lt\").\\\n",
    "        join(df_amenities_no_kg_exploded, rsuffix=\"_am\")\n",
    "    final_df.drop(columns={\"name\"}, inplace=True)\n",
    "\n",
    "    ## Convert hot encoded colums to numpy array \n",
    "    l = final_df.iloc[:,1:]\n",
    "    V = l.to_numpy(dtype=bool)\n",
    "    lookup = {}\n",
    "    for i,id in enumerate(list(final_df[\"id\"])):\n",
    "        lookup[str(id)] = V[i,:]\n",
    "    \n",
    "    return lookup, V, final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13efa036",
   "metadata": {},
   "source": [
    "### Creating a Lookup Table for Amenities\n",
    "\n",
    "The function `create_lookup_amenities_tao()` takes a prepared dataset and creates a lookup table for amenities. It first performs one-hot encoding on the listing type columns, then uses a MultiLabelBinarizer to transform the 'amenities_tao_v' column into multiple binary columns. It then joins these new columns to the original dataset, removes the 'name' column, and converts the hot-encoded columns to a numpy array. Finally, it creates a lookup dictionary where each key is a listing id and the value is the corresponding row from the numpy array. The function returns this lookup dictionary, the numpy array, the final dataframe, and the number of columns in the amenities dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c02305-7363-44c8-9c94-f9fc228649fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_amenities_tao(prepared_data):\n",
    "    X_full = prepared_data[\"df_data\"]\n",
    "    df_listing_type_exploded = pd.get_dummies(X_full[listing_type_cols], columns=listing_type_cols) ## sf with listing type hot encoding\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df_amenities_tao_exploded = pd.DataFrame(mlb.fit_transform(X_full['amenities_tao_v']), columns=mlb.classes_, index=X_full.index)\n",
    "    he_am_size = df_amenities_tao_exploded.shape[1]\n",
    "    final_df = X_full[[\"id\",\"name\"]].copy()\n",
    "    final_df = final_df.\\\n",
    "        join(df_listing_type_exploded, rsuffix=\"_lt\").\\\n",
    "        join(df_amenities_tao_exploded, rsuffix=\"_am\")\n",
    "    final_df.drop(columns={\"name\"}, inplace=True)\n",
    "\n",
    "    ## Convert hot encoded colums to numpy array \n",
    "    l = final_df.iloc[:,1:]\n",
    "    V = l.to_numpy(dtype=bool)\n",
    "    lookup = {}\n",
    "    for i,id in enumerate(list(final_df[\"id\"])):\n",
    "        lookup[str(id)] = V[i,:]\n",
    "    \n",
    "    return lookup, V, final_df, he_am_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04a643",
   "metadata": {},
   "source": [
    "### Extracting Prepared Data\n",
    "\n",
    "The code above is responsible for extracting the prepared data from the dictionary `prepared_data` and storing it in the variable `pr`. This is a simple data retrieval operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6370049-3b42-4719-95f0-d374f4e95b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = prepared_data[\"df_data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02912f5d-868b-4ed7-a4da-737053f17524",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hot encoding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f359a5a1",
   "metadata": {},
   "source": [
    "### Hot Encoding and Saving TAO Amenities\n",
    "\n",
    "The code is responsible for creating a hot encoding for amenities using the TAO mapping. It then saves this encoding to a pickle file for future use. The size of the hot encoding is also printed out for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e8acb-a607-445d-a1f9-80c2a62ead06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hot encoding for just TAO amenities \n",
    "\n",
    "vector_features=\"he_listing_types-amenities-tao\"\n",
    "output_dir=\"bert_input_data/vectors_lookup\"\n",
    "dataset_name = \"airbnb_london_20220910\"\n",
    "\n",
    "lookup_am_tao, V_am_tao, final_df, he_amenities_size = create_lookup_amenities_tao(prepared_data)\n",
    "print(\"Hot encoding size for amenities mapped to TAO:\",he_amenities_size)\n",
    "\n",
    "with open(f\"{output_dir}/all_{vector_features}_{dataset_name}_id2embedding.pickle\", 'wb') as f:\n",
    "    pickle.dump(lookup_am_tao, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd80b2",
   "metadata": {},
   "source": [
    "### Hot Encoding Vector Optimization\n",
    "\n",
    "This code is designed to optimize the size of a hot encoding vector using a range of cut-off values. It iteratively explores different cut-off points for amenity frequencies in a property dataset, aiming to achieve a hot encoding vector size that is as close as possible to a target size. The process begins with a broad step size to quickly narrow down the range, and then refines the cut-off value using a step size of 1. The final cut-off value and hot encoding size are then printed out. The initial cut-off value is set to 4, and the target hot encoding size is determined by the size of a previously defined hot encoding vector for amenities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0918417-abaf-4f0b-9641-44ee23bff086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We want to use amenities names to create a hot encoding vector with (almost) the same size as the hot encoding vector produced when mapping amenities to TAO\n",
    "\n",
    "def explore_cut_off(first_cut_off, last_cut_off, step_size, target_size):\n",
    "    active_cut_off = first_cut_off\n",
    "    for cut_off in range(first_cut_off, last_cut_off, step_size):\n",
    "        old_cut_off = active_cut_off\n",
    "        active_cut_off = cut_off\n",
    "        freq_he_size = len(property_frequencies(prepared_data[\"df_data\"]['amenities_all_v'], min_freq = active_cut_off))\n",
    "        if freq_he_size < target_size:\n",
    "            print(\"With cut off %s the hot encoding vector size is too small (%s)\" % (cut_off, freq_he_size))\n",
    "            break\n",
    "    return old_cut_off, cut_off\n",
    "\n",
    "target_he_size = he_amenities_size\n",
    "#target_he_size = 600\n",
    "#target_he_size = 701\n",
    "\n",
    "num_dist_amenities = len(property_frequencies(prepared_data[\"df_data\"]['amenities_all_v'], min_freq = 0)) ## number of distinct amenities\n",
    "print(\"total number of distinct amenities:\", num_dist_amenities)\n",
    "initial_am_num = 4\n",
    "\n",
    "first_cut_off, last_cut_off = explore_cut_off(initial_am_num, num_dist_amenities, 10, target_he_size)\n",
    "final_cut_off, _ = explore_cut_off(first_cut_off, last_cut_off, 1, target_he_size)\n",
    "\n",
    "final_he_size = len(property_frequencies(prepared_data[\"df_data\"]['amenities_all_v'], min_freq = final_cut_off))\n",
    "print(\"Cut off: %s; HE size: %s\" % (final_cut_off, final_he_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a73530",
   "metadata": {},
   "source": [
    "### Hot Encoding of Amenities\n",
    "\n",
    "The code above is used for hot encoding of frequently occurring amenities. It does this without mapping to TAO or DBPedia. The encoded features are saved in a pickle file for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e854c37f-a4da-4e86-b5e3-0cb42a8d35e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hot encoding for frequent amenities just using their names, WITHOUT mapping to TAO or DBPedia \n",
    "\n",
    "vector_features=\"he_listing_types-amenities-no-kg-\"+str(final_he_size)\n",
    "output_dir=\"bert_input_data/vectors_lookup\"\n",
    "dataset_name = \"airbnb_london_20220910\"\n",
    "## features in vector listing_types+amenities_tao+dbpedia\n",
    "lookup_am_no_kg, V_am_no_kg, final_df = create_lookup_amenities_no_kg(prepared_data, min_freq = final_cut_off)\n",
    "with open(f\"{output_dir}/all_{vector_features}_{dataset_name}_id2embedding.pickle\", 'wb') as f:\n",
    "    pickle.dump(lookup_am_no_kg, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d84678",
   "metadata": {},
   "source": [
    "### Creating and Saving Lookup Table for Amenities and TAO\n",
    "\n",
    "The code block initially specifies the vector features, output directory, and dataset name. It then creates a lookup table for amenities and TAO entities from the dataset. Finally, the lookup table is saved as a pickle file in the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8911576-dd6c-4059-8515-2b2c930b36dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Hot encoding for TAO amenities and TAO entities from listing description  \n",
    "\n",
    "vector_features=\"he_listing_types-amenities-tao_text-tao\"\n",
    "output_dir=\"bert_input_data/vectors_lookup\"\n",
    "dataset_name = \"airbnb_london_20220910\"\n",
    "\n",
    "lookup_tao, V_tao, final_df = create_lookup_amenities_tao__text_tao(prepared_data, df_tao_entities_from_text[[\"id\",\"tao_entities_v\"]])\n",
    "with open(f\"{output_dir}/all_{vector_features}_{dataset_name}_id2embedding.pickle\", 'wb') as f:\n",
    "    pickle.dump(lookup_tao, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284e860d",
   "metadata": {},
   "source": [
    "### Creating and Saving TAO Entity Vector Features\n",
    "\n",
    "The code initiates the creation of Textual Aspect of Object (TAO) entity vector features from a specific dataset. It then saves these vector features into a pickle file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48e4f63-76e0-4557-9b1c-5f9330c33a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Hot encoding for just TAO entities from listing description  \n",
    "\n",
    "vector_features=\"he_text-tao\"\n",
    "output_dir=\"bert_input_data/vectors_lookup\"\n",
    "dataset_name = \"airbnb_london_20220910\"\n",
    "\n",
    "lookup_txt_tao, V_txt_tao, final_df = create_lookup_text_tao(prepared_data, df_tao_entities_from_text[[\"id\",\"tao_entities_v\"]])\n",
    "with open(f\"{output_dir}/all_{vector_features}_{dataset_name}_id2embedding.pickle\", 'wb') as f:\n",
    "    pickle.dump(lookup_txt_tao, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607269fc",
   "metadata": {},
   "source": [
    "### Creating Vector Features and Saving to File\n",
    "\n",
    "The code first sets the necessary variables for vector features, output directory, and dataset name. Then, it creates lookup text and vectors for DBpedia entities from the prepared data. The lookup is then serialized and saved to a pickle file in the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a64b12d-3701-4394-aac9-e8bbe83c66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hot encoding for just DBpedia entities from text\n",
    "\n",
    "vector_features=\"he_dbpedia\"\n",
    "output_dir=\"bert_input_data/vectors_lookup\"\n",
    "dataset_name = \"airbnb_london_20220910\"\n",
    "\n",
    "lookup_dbp, V_dbp, final_df = create_lookup_text_dbp(prepared_data)\n",
    "with open(f\"{output_dir}/all_{vector_features}_{dataset_name}_id2embedding.pickle\", 'wb') as f:\n",
    "    pickle.dump(lookup_dbp, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b456765c",
   "metadata": {},
   "source": [
    "### Creating and Saving Vector Lookup for Amenities and Text Description\n",
    "\n",
    "The code block is responsible for creating a lookup table for hot encoded amenities and text description using TAO and DBpedia. The lookup table is then saved as a pickle file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce53ec6-8f42-4a03-8d76-48be2b6ef054",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hot encoding for TAO AND DBpedia feature\n",
    "\n",
    "## Hot encoding for amenities with TAO and description text with DBpedia\n",
    "vector_features=\"he_listing_types-amenities-tao__dbpedia\"\n",
    "output_dir=\"bert_input_data/vectors_lookup\"\n",
    "dataset_name = \"airbnb_london_20220910\"\n",
    "\n",
    "lookup_all_he, V_all_he, final_df = create_lookup_amenities_tao__text_dbp(None, prepared_data)\n",
    "with open(f\"{output_dir}/all_{vector_features}_{dataset_name}_id2embedding.pickle\", 'wb') as f:\n",
    "    pickle.dump(lookup_all_he, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ff42e",
   "metadata": {},
   "source": [
    "### Creating Hot Encoding Lookup Table and Saving to File\n",
    "\n",
    "This code block creates a lookup table for hot encoding based on the target data. It iterates over the keys of the `targets` dictionary, generates a lookup table for each target using the `create_lookup_amenities_tao__text_dbp` function, and stores the result in the `lookup_he_by_target` dictionary. It then saves each lookup table to a pickle file in the `bert_input_data/vectors_lookup` directory for future use. The filename of the pickle file is composed of the target name, vector features, and the dataset name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa373fa-15bb-4525-ab16-bee3c08eee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Hot encoding divided by target data: used for backward compatibility\n",
    "lookup_he_by_target = {}\n",
    "for t in targets.keys():\n",
    "    vector_features=\"he_listing_types-amenities-tao__dbpedia\"\n",
    "    output_dir=\"bert_input_data/vectors_lookup\"\n",
    "    dataset_name = \"airbnb_london_20220910\"\n",
    "\n",
    "    lookup, V, final_df = create_lookup_amenities_tao__text_dbp(targets[t][\"balanced_data\"], prepared_data)\n",
    "    lookup_he_by_target[t] = lookup\n",
    "    with open(f\"{output_dir}/{t}_{vector_features}_{dataset_name}_id2embedding.pickle\", 'wb') as f:\n",
    "        pickle.dump(lookup, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a2da2a",
   "metadata": {},
   "source": [
    "### Loading Pickle File\n",
    "\n",
    "The code opens and reads a pickle file which contains embeddings for Airbnb London listings. These embeddings are then stored in the variable 'content'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd182c1c-7251-419a-8ab9-943a2388b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bert_input_data/vectors_lookup/price_he_listing_types-amenities-tao__dbpedia_airbnb_london_20220910_id2embedding.pickle\", 'rb') as pickle_file:\n",
    "    content = pickle.load(pickle_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91aa26d",
   "metadata": {},
   "source": [
    "### Hot Encoding for TAO and DBpedia Features\n",
    "\n",
    "This code performs hot encoding for TAO and DBpedia features. \n",
    "\n",
    "##### Compact Hot Encoding for All Data\n",
    "The first section of the code performs hot encoding for all data. It creates a lookup table for the compact hot encoding, then saves this lookup table to a pickle file for future use.\n",
    "\n",
    "##### Compact Hot Encoding Divided by Target Data\n",
    "The second section of the code performs hot encoding for each target in the target data. It creates a lookup table for each target, then saves these lookup tables to individual pickle files for future use. This is done for backward compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e965b49-1d43-45a0-a13f-daf813e91243",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compact hot encoding for TAO AND DBpedia feature\n",
    "\n",
    "##### Compact hot encoding for all data\n",
    "vector_features=\"listing_types-amenities-tao__dbpedia\"\n",
    "output_dir=\"bert_input_data/vectors_lookup\"\n",
    "dataset_name = \"airbnb_london_20220910\"\n",
    "## features in vector listing_types+amenities_tao+dbpedia\n",
    "lookup_all_he_compact, V_all_compact, _ = create_lookup_compact(None, prepared_data)\n",
    "with open(f\"{output_dir}/all_{vector_features}_{dataset_name}_id2embedding.pickle\", 'wb') as f:\n",
    "    pickle.dump(lookup_all_he_compact, f)\n",
    "\n",
    "##### Compact hot encoding divided by target data: used for backward compatibility\n",
    "lookup_compact_he_by_target = {}\n",
    "for t in targets.keys():\n",
    "    vector_features=\"listing_types-amenities-tao__dbpedia\"\n",
    "    output_dir=\"bert_input_data/vectors_lookup\"\n",
    "    dataset_name = \"airbnb_london_20220910\"\n",
    "    ## features in vector listing_types+amenities_tao+dbpedia\n",
    "    lookup, V, _ = create_lookup_compact(targets[t][\"balanced_data\"], prepared_data)\n",
    "    lookup_compact_he_by_target[t] = lookup\n",
    "    with open(f\"{output_dir}/{t}_{vector_features}_{dataset_name}_id2embedding.pickle\", 'wb') as f:\n",
    "        pickle.dump(lookup, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1055aa8",
   "metadata": {},
   "source": [
    "### Extracting and Comparing Key Sets from Different Data Sources\n",
    "\n",
    "The code above creates sets of keys from different lookup files (`lookup_tao`, `lookup_he_by_target`, `lookup_compact_he_by_target`). It then compares these keys to the keys found in the `targets` dictionary to ensure all required IDs are present across all data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c0cecd-ea95-47e7-81c1-a5a348a31252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Check if we have all ids found in lookup file for single targets compared to to those in lookup files created from all prepared data\n",
    "\n",
    "all_keys=set(lookup_tao.keys())\n",
    "all_keys_he_from_targets = set()\n",
    "all_keys_he_compact_from_targets = set()\n",
    "for t in targets.keys():\n",
    "    all_keys_he_from_targets = all_keys.union(set(lookup_he_by_target[t]))\n",
    "    all_keys_he_compact_from_targets = all_keys.union(set(lookup_compact_he_by_target[t]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cbf510",
   "metadata": {},
   "source": [
    "### Compatibility Verification\n",
    "\n",
    "The code is performing a check for retro compatibility. It asserts that all ids from the prepared data cover all ids taken from single targets and potentially more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe92bb-7165-4e8b-8c00-1c1688b7284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## retro compatibility check\n",
    "## if we take all ids from prepared data we cover all id taken form single targets and more\n",
    "assert len(all_keys - all_keys_he_from_targets) == 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bad38bd",
   "metadata": {},
   "source": [
    "### Checking Retro Compatibility\n",
    "\n",
    "The code snippet is asserting that there is full retro compatibility between two sets of keys, `all_keys` and `all_keys_he_compact_from_targets`. It ensures that there are no extra keys in `all_keys` that are not present in `all_keys_he_compact_from_targets`, validating the consistency of data between the two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3368029-c69f-4154-b8f0-4ca7d579a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "## retro compatibility check\n",
    "## if we take all ids from prepared data we cover all id taken form single targets and more\n",
    "assert len(all_keys - all_keys_he_compact_from_targets) == 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f1181",
   "metadata": {},
   "source": [
    "### Data Consistency Verification\n",
    "\n",
    "This section of the code performs a sanity check to ensure that the combined length of the 'lookup_am_tao' and 'lookup_dbp' dataframes for a specific index my_key equals the length of the 'lookup_all_he' dataframe for the same index. This is done to confirm data consistency across different dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fee866-d7cb-4d24-a52f-4be988e8e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sanity check\n",
    "## the dbpedia he + tao he shoud have the same length as the complete dbpedia + tao he\n",
    "my_key = list(lookup_am_tao.keys())[0]\n",
    "assert lookup_am_tao[my_key].shape[0] + lookup_dbp[my_key].shape[0] == lookup_all_he[my_key].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d733a-8125-4a05-b4d4-695245b63e23",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare train/dev/test for KGE-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fff1cb7",
   "metadata": {},
   "source": [
    "### Cleaning and Sorting Entities Function\n",
    "\n",
    "This function drops missing values from the input DataFrame, extracts the 'surface_form' and 'pos' values from the 'entity_data' column, removes duplicate entries based on 'source_id', 'uri', and 'surface_form', and then sorts the DataFrame by 'source_id' and 'pos'. The function finally drops the 'pos' and 'surface_form' columns before returning the cleaned and sorted DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a25c13-3f91-43b0-a2cd-d611ffd9c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_sort_entities(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    df[\"surface_form\"] = df.entity_data.apply(lambda e: e[\"surface_form\"])\n",
    "    df.drop_duplicates([\"source_id\", \"uri\", \"surface_form\"], inplace=True)\n",
    "    df[\"pos\"] = df.entity_data.apply(lambda e: e['surface_char_pos'][0])\n",
    "    df = df.sort_values(by=[\"source_id\", \"pos\"]).drop(columns=[\"pos\",\"surface_form\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b2e002",
   "metadata": {},
   "source": [
    "### Preparing DataFrame for BERT Augmentation\n",
    "\n",
    "This function prepares a DataFrame for BERT augmentation. It creates a new DataFrame with specific columns from the input DataFrames, including text, title, authors, over, under, and metadata columns. The \"over\" and \"under\" columns are derived from the Y_df DataFrame. The function then returns the newly created DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ac32e-fa58-4312-aeab-f1d5660d48fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bert_augmented_df(X_df, Y_df, metadata_cols, text_col):\n",
    "    df = pd.DataFrame()\n",
    "    df[\"text\"] = X_df[text_col] #.apply(lambda txt: pre_process(txt))\n",
    "    df[\"title\"] = X_df[\"name\"]\n",
    "    df[\"authors\"] = X_df[\"id\"].astype(str)\n",
    "    df[\"over\"] = Y_df*1\n",
    "    df[\"under\"] = 1-Y_df*1\n",
    "    df[metadata_cols]=X_df[metadata_cols]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f07eb6d",
   "metadata": {},
   "source": [
    "### Saving BERT Augmented Data\n",
    "\n",
    "The function `save_bert_augmented_data_as_pickle` prepares and saves the training, development, and testing datasets into pickle files. The data is prepared using the `prepare_bert_augmented_df` function, and then stored in the specified output directory. The function returns the prepared dataframes for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900042a6-35f4-4079-8246-f85d2594a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bert_augmented_data_as_pickle(target_name, dataset_name, \n",
    "                                       X_train_df, Y_train_df, \n",
    "                                       X_dev_df, Y_dev_df,\n",
    "                                       X_test_df, Y_test_df,\n",
    "                                       text_col, metadata_cols, metadata_type_label, output_dir):\n",
    "    \n",
    "    train_df = prepare_bert_augmented_df(X_train_df, Y_train_df,metadata_cols, text_col)\n",
    "    dev_df = prepare_bert_augmented_df(X_dev_df, Y_dev_df, metadata_cols, text_col)\n",
    "    test_df = prepare_bert_augmented_df(X_test_df, Y_test_df, metadata_cols, text_col)\n",
    "    labels = [\"over\", \"under\"]\n",
    "    \n",
    "    train_output_data = (train_df,metadata_cols,[],labels)\n",
    "    with open(f\"{output_dir}/train_{metadata_type_label}{target_name}_{dataset_name}.pickle\", 'wb') as f:\n",
    "        pickle.dump(train_output_data, f)\n",
    "\n",
    "    dev_output_data = (dev_df,metadata_cols,[],labels)\n",
    "    with open(f\"{output_dir}/dev_{metadata_type_label}{target_name}_{dataset_name}.pickle\", 'wb') as f:\n",
    "        pickle.dump(dev_output_data, f)\n",
    "    \n",
    "    test_output_data = (test_df,metadata_cols,[],labels)\n",
    "    with open(f\"{output_dir}/test_{metadata_type_label}{target_name}_{dataset_name}.pickle\", 'wb') as f:\n",
    "        pickle.dump(test_output_data, f)\n",
    "    \n",
    "    return train_df, dev_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d512f",
   "metadata": {},
   "source": [
    "### Data Preparation and Cleanup\n",
    "\n",
    "1. The variable `t` is set to \"price\".\n",
    "2. A dictionary `metadata_type_labels` is defined to map metadata types to labels.\n",
    "3. A copy of the \"balanced_data\" from the \"price\" target is made and stored in `X_full`.\n",
    "4. The \"description\" column of `X_full` is copied to a new column \"text_a\".\n",
    "5. Rows with empty descriptions are removed from `X_full`.\n",
    "6. Metadata type is set to \"base_cols\".\n",
    "7. A list of metadata columns is extracted from the \"price\" target.\n",
    "8. Unwanted columns 'id', 'scrape_id', 'host_id' are removed from the metadata columns list.\n",
    "9. The label for the metadata type is retrieved from the `metadata_type_labels` dictionary.\n",
    "10. The target variable `y` is extracted from the \"price\" target.\n",
    "11. A new dataframe `kbert_format_df` is created by selecting specific columns from `X_full` and joining with `y`, resetting the index, and renaming the \"__y__\" column to \"label\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb7255-a493-45b1-85b1-e4a0b49ab416",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"price\"\n",
    "metadata_type_labels = { \"base_cols\": \"\", \"numeric_cols\":\"all_meta_\"}\n",
    "X_full = targets[t][\"balanced_data\"].copy()\n",
    "X_full[\"text_a\"] = X_full[\"description\"] #.apply(lambda t: t if len(t) <= max_length else t[0:max_length])\n",
    "X_full = X_full[X_full.description !=\"\"]\n",
    "metadata_type = \"base_cols\"\n",
    "metadata_cols = list(targets[t][metadata_type])\n",
    "metadata_cols = [col for col in metadata_cols if col not in ['id', 'scrape_id', 'host_id']] ## remove unwanted colums if present\n",
    "metadata_type_label = metadata_type_labels[metadata_type]\n",
    "y = targets[t][\"y\"]\n",
    "kbert_format_df = X_full[[\"id\",\"text_a\",\"name\"] + metadata_cols].join(y)\\\n",
    "    .reset_index(drop=True)\\\n",
    "    .rename(columns={\"__y__\":\"label\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14f56c6",
   "metadata": {},
   "source": [
    "### Description Enhancement Functions\n",
    "\n",
    "The first four functions (`extend_description`, `extend_description_v2`, `extend_description_with_lodging_type`, `extend_description_with_lodging_type_v2`) are used to extend the description of a property with its amenities and lodging type.\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "The code then copies the balanced data from the target to `X_full`. It finds the most frequent amenities in all listings and stores them in `frequent_amenities`. For each listing, it only stores the amenities that appear in `frequent_amenities`. \n",
    "\n",
    "### Description Extension Application\n",
    "\n",
    "Finally, it applies the `extend_description` function to each row in `X_full`, extending each description with its frequent amenities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544489d3-1b59-43c2-88a7-1c5611969ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_description(description, amenities):\n",
    "    new_description = \"The following amenities are included: \"\n",
    "    for amenity in amenities:\n",
    "        new_description = new_description + amenity + \", \"\n",
    "    return description + new_description\n",
    "\n",
    "def extend_description_v2(description, amenities):\n",
    "    new_description = \"\"\n",
    "    for amenity in amenities:\n",
    "        new_description = new_description + amenity + \" \"\n",
    "    return description + new_description\n",
    "\n",
    "def extend_description_with_lodging_type(description, amenities, property_type, room_type):\n",
    "    new_description = extend_description(description, amenities) + \" \"\n",
    "    listing_type_description = \"We offer: \"+ property_type + \" \" + room_type + \" \"\n",
    "    return pre_process(listing_type_description + new_description)\n",
    "\n",
    "def extend_description_with_lodging_type_v2(description, amenities, property_type, room_type):\n",
    "    new_description = extend_description_v2(description, amenities) + \" \"\n",
    "    listing_type_description = \" \"+ property_type + \" \" + room_type + \" \"\n",
    "    return pre_process(new_description + listing_type_description)\n",
    "\n",
    "X_full = targets[t][\"balanced_data\"].copy()\n",
    "frequent_amenities = property_frequencies(X_full['amenities_all_v'], min_freq = final_cut_off) ## find the amenities used more than min_freq in all listings\n",
    "X_full['amenities_freq_v'] = X_full['amenities_all_v'].apply(lambda am_list: list(set(am_list).intersection(frequent_amenities))) ## for each listing only store the amenities in the frequent_amenity list\n",
    "\n",
    "X_full['ext_description'] = X_full.apply(lambda row: extend_description(row.description, row.amenities_freq_v), axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d93cb",
   "metadata": {},
   "source": [
    "### Function: numeric_metadata_to_text\n",
    "\n",
    "This function concatenates the values of metadata columns with a description column into a single string. It iterates over each metadata column, converting the numeric metadata into a string and appending it to the description text.\n",
    "\n",
    "### Function: numeric_metadata_to_text_v2\n",
    "\n",
    "This function is a variant of the previous function. It also concatenates the values of metadata columns with a description column into a single string, but it omits the column names in the final text. It simply appends the numeric metadata values to the description text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58272b96-0754-44c2-b686-6c995db011c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_metadata_to_text(row, metadata_cols, description_col):\n",
    "    text = row[description_col] + \" other features are \"\n",
    "    for col in metadata_cols:\n",
    "        text = text + col + \" is \" + str(row[col]) + \" \"\n",
    "    return text\n",
    "\n",
    "def numeric_metadata_to_text_v2(row, metadata_cols, description_col):\n",
    "    text = row[description_col] + \" \"\n",
    "    for col in metadata_cols:\n",
    "        text = text + \" \" + str(row[col]) + \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c324468",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Saving for BERT Augmentation\n",
    "\n",
    "This script handles the data preprocessing for the Airbnb dataset, specifically for BERT augmentation with Knowledge Graphs (KGE-BERT). It performs the following tasks:\n",
    "\n",
    "1. Defines the necessary parameters and directories.\n",
    "2. Iterates over the keys of the `targets` dictionary, which contains the different target variables for the model.\n",
    "3. For each target, it performs several preprocessing steps such as metadata processing, removing unwanted columns, extending the description with a list of amenities, and normalizing metadata if required.\n",
    "4. It then splits the processed data into train, dev, and test sets.\n",
    "5. The script also extends the textual description of the listings with the list of amenities and the type of lodging, and converts numeric metadata to text.\n",
    "6. The processed data is then saved in different formats depending on the type of extension (plain, extended with amenities, extended with amenities and lodging type, etc).\n",
    "7. Finally, it prints out the number of columns and samples in the train, dev, and test sets for each type of metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bef4e3-a717-46ed-a4a8-ab2db9054021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev_size = 0.1\n",
    "test_size = 0.1\n",
    "max_length = 512\n",
    "dev_test_size = dev_size + test_size\n",
    "test_size_fraction = test_size / dev_test_size\n",
    "#dataset_name = \"airbnb_london_20220910\" ### Defined at the start of the notebook\n",
    "metadata_type_labels = { \"base_cols\": \"\", \"numeric_cols\":\"all_meta_\"}\n",
    "bert_augmented_output_dir = \"bert_input_data\" ## output dir for bert augmented with KG\n",
    "kbert_output_dir = \"kbert_input_data\"    \n",
    "\n",
    "#t = \"price\"\n",
    "for t in targets.keys(): #[\"price\"]: #\n",
    "    print(\"Target: \", t)\n",
    "    print(\"----------Bert augmented with kg-----------\")\n",
    "    \n",
    "    for metadata_directive in [\"base_cols\", \"base_cols:norm\", \"numeric_cols\", \"numeric_cols:normalize\"]:\n",
    "    #for metadata_directive in [\"base_cols:norm\", \"numeric_cols:norm\"]:\n",
    "    #for metadata_directive in [\"base_cols:norm\"]:\n",
    "        \n",
    "        metadata_type = metadata_directive.split(\":\")[0]\n",
    "        try:\n",
    "            metadata_processing = metadata_directive.split(\":\")[1]+\"_\"\n",
    "            print(\"Metadata processing method: \", metadata_processing)\n",
    "            \n",
    "        except:\n",
    "            metadata_processing = \"\"\n",
    "            print(\"No processing of metadata\")\n",
    "            \n",
    "        #metadata_cols = list(targets[t][\"base_cols\"])\n",
    "        metadata_cols = list(targets[t][metadata_type])\n",
    "        metadata_cols = [col for col in metadata_cols if col not in ['id', 'scrape_id', 'host_id']] ## remove unwanted colums if present\n",
    "        metadata_type_label = metadata_type_labels[metadata_type] + metadata_processing ## extend metadata_type_label with the processing label\n",
    "        \n",
    "        print(\"Metadata type label:\", metadata_type_label)\n",
    "\n",
    "        X_full = targets[t][\"balanced_data\"].copy()\n",
    "        X_full[\"text_a\"] = X_full[\"description\"] #.apply(lambda t: t if len(t) <= max_length else t[0:max_length])\n",
    "        X_full = X_full[X_full.description !=\"\"]\n",
    "        \n",
    "        ### change column amenities_freq_v contains only the most frequent amenities so that the hot encoding vector size has size final_he_size\n",
    "        frequent_amenities = property_frequencies(X_full['amenities_all_v'], min_freq = final_cut_off) ## find the amenities used more than min_freq in all listings\n",
    "        X_full['amenities_freq_v'] = X_full['amenities_all_v'].apply(lambda am_list: list(set(am_list).intersection(frequent_amenities))) ## for each listing only store the amenities in the frequent_amenity list\n",
    "        \n",
    "        \n",
    "        \n",
    "        X_full['ext_description'] = X_full.apply(lambda row: extend_description(row.description, row.amenities_freq_v), axis=1) ## extend the textual description with the list of amenities\n",
    "        X_full['ext_description_with_lodging_type'] = X_full.apply(\n",
    "            lambda row: extend_description_with_lodging_type(row.description, row.amenities_freq_v, row.property_type, row.room_type), axis=1) ## extend the textual description with the list of amenities\n",
    "        X_full['ext_description_with_lodging_type_v2'] = X_full.apply(\n",
    "            lambda row: extend_description_with_lodging_type_v2(row.description, row.amenities_freq_v, row.property_type, row.room_type), axis=1) ## extend the textual description with the list of amenities\n",
    "\n",
    "        X_full['ext_tao_description_with_lodging_type'] = X_full.apply(\n",
    "            lambda row: extend_description_with_lodging_type(row.description, row.amenities_tao_v, row.property_type, row.room_type), axis=1) ## extend the textual description with the list of amenities\n",
    "        X_full['ext_tao_description_with_lodging_type_v2'] = X_full.apply(\n",
    "            lambda row: extend_description_with_lodging_type_v2(row.description, row.amenities_tao_v, row.property_type, row.room_type), axis=1) ## extend the textual description with the list of amenities\n",
    "\n",
    "        X_full['ext_description_and_meta_with_lodging_type'] = X_full.apply(lambda row: numeric_metadata_to_text(row, metadata_cols, 'ext_description_with_lodging_type'), axis=1)\n",
    "        X_full['ext_description_and_meta_with_lodging_type_v2'] = X_full.apply(lambda row: numeric_metadata_to_text_v2(row, metadata_cols, 'ext_description_with_lodging_type_v2'), axis=1)\n",
    "        X_full['ext_tao_description_and_meta_with_lodging_type'] = X_full.apply(lambda row: numeric_metadata_to_text(row, metadata_cols, 'ext_tao_description_with_lodging_type'), axis=1)\n",
    "        X_full['ext_tao_description_and_meta_with_lodging_type_v2'] = X_full.apply(lambda row: numeric_metadata_to_text_v2(row, metadata_cols, 'ext_tao_description_with_lodging_type_v2'), axis=1)\n",
    "        \n",
    "        # if metadata_processing == \"norm_\": ### we have to normalize metadata\n",
    "        #     print(\"Normalizing metadata\")\n",
    "        #     X_full[metadata_cols] = normalize(X_full[metadata_cols], norm='l2')            \n",
    "        #     break\n",
    "        \n",
    "\n",
    "        y = targets[t][\"y\"]\n",
    "        kbert_format_df = X_full[[\n",
    "                \"id\",\"text_a\",\"name\", \n",
    "                \"ext_description\", \n",
    "                'ext_description_with_lodging_type', \n",
    "                'ext_description_with_lodging_type_v2', \n",
    "                'ext_tao_description_with_lodging_type',\n",
    "                'ext_tao_description_with_lodging_type_v2',\n",
    "                'ext_description_and_meta_with_lodging_type',\n",
    "                'ext_description_and_meta_with_lodging_type_v2',\n",
    "                'ext_tao_description_and_meta_with_lodging_type',\n",
    "                'ext_tao_description_and_meta_with_lodging_type_v2'] + metadata_cols].join(y)\\\n",
    "            .reset_index(drop=True)\\\n",
    "            .rename(columns={\"__y__\":\"label\"})\n",
    "\n",
    "        kbert_format_df[[\"label\"]] = kbert_format_df[[\"label\"]] * 1\n",
    "        X_train, X_dev_test, y_train, y_dev_test = train_test_split(kbert_format_df, kbert_format_df[\"label\"], test_size=dev_test_size, random_state=DEFAULT_RANDOM_STATE, shuffle=True)\n",
    "        X_dev, X_test, y_dev, y_test = train_test_split(X_dev_test, y_dev_test, test_size=test_size_fraction, random_state=DEFAULT_RANDOM_STATE, shuffle=True)\n",
    "\n",
    "        \n",
    "    \n",
    "        if metadata_processing == \"norm_\": ### we have to normalize metadata\n",
    "            print(\"Normalizing metadata\")\n",
    "            \n",
    "            # We train the scaler using train data to avoid leaking of informations to dev and test splits\n",
    "            # We use MinMaxScaler to have all data value in the range [0,1] to be comparable with hot encoding\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(X_train[metadata_cols])\n",
    "            #print(\"Scaler data max: \", scaler.data_max_)\n",
    "            \n",
    "            #X_train[metadata_cols] = pd.DataFrame(normalize(X_train[metadata_cols], norm='l2'))\n",
    "            X_train[metadata_cols] = scaler.transform(X_train[metadata_cols])\n",
    "            X_dev[metadata_cols] = scaler.transform(X_dev[metadata_cols])\n",
    "            X_test[metadata_cols] = scaler.transform(X_test[metadata_cols])\n",
    "            \n",
    "        \n",
    "        ### Save dataset with plain description extracted from AirBnB data\n",
    "        bert_kg_train_df, bert_kg_dev_df, bert_kg_test_df = save_bert_augmented_data_as_pickle(t, dataset_name, \n",
    "                                                                                               X_train, y_train, X_dev, y_dev, X_test, y_test, \n",
    "                                                                                        \"text_a\", metadata_cols, metadata_type_label, bert_augmented_output_dir)\n",
    "\n",
    "        ### Save dataset where the description extracted from AirBnB data is extended with a list of included amenities\n",
    "        bert_kg_train_ext_df, bert_kg_dev_ext_df, bert_kg_test_ext_df = save_bert_augmented_data_as_pickle(t+\"_extended\", dataset_name, \n",
    "                                                                                               X_train, y_train, X_dev, y_dev, X_test, y_test, \n",
    "                                                                                        \"ext_description\", metadata_cols, metadata_type_label, bert_augmented_output_dir)\n",
    "\n",
    "        ### Save dataset where the description extracted from AirBnB data is extended with a list of included amenities and the listing type\n",
    "        bert_kg_train_ext_lt_df, bert_kg_dev_ext_lt_df, bert_kg_test_ext_lt_df = save_bert_augmented_data_as_pickle(t+\"_extended_lt\", dataset_name, \n",
    "                                                                                               X_train, y_train, X_dev, y_dev, X_test, y_test, \n",
    "                                                                                        \"ext_description_with_lodging_type\", metadata_cols, metadata_type_label, bert_augmented_output_dir)\n",
    "        ### 2nd version. Save dataset where the description extracted from AirBnB data is extended with a list of included amenities and the listing type\n",
    "        bert_kg_train_ext_lt_df, bert_kg_dev_ext_lt_df, bert_kg_test_ext_lt_df = save_bert_augmented_data_as_pickle(t+\"_extended_lt_v2\", dataset_name, \n",
    "                                                                                               X_train, y_train, X_dev, y_dev, X_test, y_test, \n",
    "                                                                                        \"ext_description_with_lodging_type_v2\", metadata_cols, metadata_type_label, bert_augmented_output_dir)\n",
    "\n",
    "        \n",
    "        ### Save dataset where the description extracted from AirBnB data is extended with a list of included amenities (mapped to TAO) and the listing type\n",
    "        bert_kg_train_tao_ext_lt_df, bert_kg_dev_tao_ext_lt_df, bert_kg_test_tao_ext_lt_df = save_bert_augmented_data_as_pickle(t+\"_extended_tao_lt\", dataset_name, \n",
    "                                                                                               X_train, y_train, X_dev, y_dev, X_test, y_test, \n",
    "                                                                                        \"ext_tao_description_with_lodging_type\", metadata_cols, metadata_type_label, bert_augmented_output_dir)\n",
    "        \n",
    "        ### 2nd version. Save dataset where the description extracted from AirBnB data is extended with a list of included amenities (mapped to TAO) and the listing type\n",
    "        bert_kg_train_tao_ext_lt_df, bert_kg_dev_tao_ext_lt_df, bert_kg_test_tao_ext_lt_df = save_bert_augmented_data_as_pickle(t+\"_extended_tao_lt_v2\", dataset_name, \n",
    "                                                                                               X_train, y_train, X_dev, y_dev, X_test, y_test, \n",
    "                                                                                        \"ext_tao_description_with_lodging_type_v2\", metadata_cols, metadata_type_label, bert_augmented_output_dir)\n",
    "               \n",
    "        ### Save dataset where the description extracted from AirBnB data is extended with a list of included amenities and the listing type\n",
    "        bert_kg_train_ext_meta_lt_df, bert_kg_dev_ext_meta_lt_df, bert_kg_test_ext_meta_lt_df = \\\n",
    "                    save_bert_augmented_data_as_pickle(t+\"_extended_meta_lt\", dataset_name, \n",
    "                                                           X_train, y_train, X_dev, y_dev, X_test, y_test, \n",
    "                                                            \"ext_description_and_meta_with_lodging_type\", metadata_cols, metadata_type_label, bert_augmented_output_dir)\n",
    "\n",
    "        ### 2nd version. Save dataset where the description extracted from AirBnB data is extended with a list of included amenities and the listing type\n",
    "        bert_kg_train_ext_meta_lt_df, bert_kg_dev_ext_meta_lt_df, bert_kg_test_ext_meta_lt_df = \\\n",
    "                    save_bert_augmented_data_as_pickle(t+\"_extended_meta_lt_v2\", dataset_name, \n",
    "                                                           X_train, y_train, X_dev, y_dev, X_test, y_test, \n",
    "                                                            \"ext_description_and_meta_with_lodging_type_v2\", metadata_cols, metadata_type_label, bert_augmented_output_dir)\n",
    "\n",
    "\n",
    "        ### Save dataset where the description extracted from AirBnB data is extended with a list of included amenities (mapped to TAO) and the listing type\n",
    "        bert_kg_train_tao_ext_meta_lt_df, bert_kg_dev_tao_ext_meta_lt_df, bert_kg_test_tao_ext_meta_lt_df = \\\n",
    "                    save_bert_augmented_data_as_pickle(t+\"_extended_tao_meta_lt\", dataset_name, \n",
    "                                                           X_train, y_train, X_dev, y_dev, X_test, y_test, \n",
    "                                                            \"ext_tao_description_and_meta_with_lodging_type\", metadata_cols, metadata_type_label, bert_augmented_output_dir)\n",
    "        \n",
    "        ### 2nd version. Save dataset where the description extracted from AirBnB data is extended with a list of included amenities (mapped to TAO) and the listing type\n",
    "        bert_kg_train_tao_ext_meta_lt_df, bert_kg_dev_tao_ext_meta_lt_df, bert_kg_test_tao_ext_meta_lt_df = \\\n",
    "                    save_bert_augmented_data_as_pickle(t+\"_extended_tao_meta_lt_v2\", dataset_name, \n",
    "                                                           X_train, y_train, X_dev, y_dev, X_test, y_test, \n",
    "                                                            \"ext_tao_description_and_meta_with_lodging_type_v2\", metadata_cols, metadata_type_label, bert_augmented_output_dir)\n",
    "\n",
    "        \n",
    "        print(\"----------Metadata: %s -----------\" % metadata_type)\n",
    "        print(\"Num columns:\", bert_kg_train_df.shape[1])\n",
    "        print(\"Train samples:\", bert_kg_train_df.shape[0])\n",
    "        print(\"Dev samples:\", bert_kg_dev_df.shape[0])    \n",
    "        print(\"Test samples:\", bert_kg_test_df.shape[0])      \n",
    "        print(\"\\n\\n\")   \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa790a3f",
   "metadata": {},
   "source": [
    "### Plotting Bar Charts for Targets\n",
    "\n",
    "The code iterates through a dictionary of targets, retrieving the target values and associated parameters for each target. It then generates a bar chart visualizing the distribution of target values that are above and below a specified threshold. The code also prints the shape of the target values and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f910bf9f-3f82-4455-ae2c-6b2238b374fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for target_name in targets.keys():\n",
    "    #target_name = \"price\"\n",
    "    target_values = targets[target_name][\"y\"]\n",
    "    THRESHOLD = targets[target_name][\"threshold\"]\n",
    "    SAFE = targets[target_name][\"safe\"]\n",
    "    plt.bar([target_name+\" under threshold (%.2f)\" % (THRESHOLD - SAFE) , target_name+\" over threshold (%.2f)\" % (THRESHOLD + SAFE)],target_values.value_counts())\n",
    "    plt.title('%s balanced data' % target_name)\n",
    "    plt.show()\n",
    "    print(target_name, target_values.shape)\n",
    "    print(target_values.value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
